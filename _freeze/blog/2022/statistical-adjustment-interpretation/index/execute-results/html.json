{
  "hash": "3b009a5d7bbe4adcc1f92d0b7f0692d5",
  "result": {
    "markdown": "---\ntitle: \"These Aren't the Effects You're Looking For\"\ndate: 2022-12-22\ndescription: \"This blog post provides an overview of the logic of statistical control, the fallacy of mutual adjustment, and a simulation-based illustration of correct and incorrect approaches to the interpretation of multivariable regression models in the social sciences.\"\ntoc: true\ntoc-location: body\ntoc-depth: 3\ncitation: true\ncode-tools: true\nresources: \n  - \"data/sim_draws_summ.rds\"\ncategories:\n  - R\n  - Bayesian Statistics\n  - Causal Inference\n  - Statistics\n  - Stan\n  - Simulation\n---\n\n\n<style>\n.column { padding-right: 1ex }\n.column + .column { padding-left: 1ex }\n</style>\n\n$$\n\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n$$\n\n\n\n\n\n::: {.callout-caution}\n## Notes About Assumptions\n\nThis post assumes a basic familiarity with Rubin's [-@Rubin1974; -@Rubin1976; -@Rubin2005] \npotential outcomes framework and the general logic of directed acyclic graphs. For an \nintroductory overview of the logic of causal inference in observational settings that \nmay provide helpful background, I direct readers to @Rohrer2018 and @Cinelli2022.\n::: \n\n## Introduction\n\n@Westreich2013 originally coined the term Table 2 fallacy to describe the common \npractice of presenting confounders included in a regression model alongside the \ntreatment or exposure of interest in Table 2 of an article and tendency of \nresearchers to interpret the coefficients for said confounders as total \neffect estimates. Subsequent work has further highlighted the problem in political \nscience [@Keele2019], economics [@Huenermund2020], and sociology [@Lundberg2021]. \nThe consequences of this practice are often pernicious, with subsequent studies \ntreating inappropriately reported estimates as theoretically informative quantities \ndespite having no valid interpretation in their original context and readers \ndrawing incorrect conclusions with no empirical basis. Despite efforts to discourage \nthe practice and some steps in the right direction, the Table 2 fallacy remains an \noften committed mistake by researchers across a diverse range of fields.\n\n::: a\nThis blog post provides an overview of the logic of statistical control, the \nfallacy of mutual adjustment, and a simulation-based illustration of correct\nand incorrect approaches to the interpretation of multivariable regression\nmodels in the social sciences. In this post I take as my starting point two \nprimary assumptions about research in the quantitative social sciences. \nFirst, I assume the goal of any scientific study that does not explicitly \nstate otherwise is to evaluate some proposed theoretical explanation for an \nassumed causal relationship. Research in the contemporary social sciences\nis primarily interested in evaluating theoretical claims about causal \nprocesses and the practice of relying on euphamisms and weasel words\nto avoid saying *cause* and *effect* while still heavily implying the \nexistance of a causal relationship does not change this reality \n[@Samii2016; @Hernan2018].\n\nSecond, I assume that any theoretical process from which testable\nimplications can be derived may be represented in the form of a directed \nacyclic graph (DAG). While DAGs may be more common in some fields than\nothers, the ability to express an assumed theoretical process in the form\nof a non-parameteric causal graph is a necessary condition for specifying\na model of the process and estimating a causal relationship. If this is\nnot possible, it generally implies the researcher is confused about\nwhat exactly they are trying to accomplish and does not have a properly \ndefined research question [@Lundberg2021].\n\nNeither of these assumptions should be taken to imply all research\nin the social sciences must be concerned with evaluating causal claims. \nIndeed, there is value in *mere description* and exploratory research that \nhighlights interesting questions and provides useful context for causal\nanalyses or theory building [@Gerring2004; @Gerring2012]. It is, however,\nnecessary to be explicit about our inferential goals and the question we\nare attempting to answer in a given study. After all, the foundation of\nsocial scientific inquiry and quantitative research more broadly lies in \nthe art of providing vague answers to precise questions.\n::: \n\n## The Logic of Regression Adjustment\n\nFollowing Rubin [-@Rubin1974; @Rubin1976; @Rubin2005], a causal effect is\ndefined as the difference in *potential outcomes*. Letting $Y_{i}$ represent\nthe observed outcome for each unit $i \\in \\{1,2,\\dots, N\\}$, $X_{i}$ the \nobserved treatment status for the $i^{th}$ unit, and $Z$ some set of\nmeasured confounders that influence both the treatment assignment mechanism\nand the observed outcome we can express the causal effect of a binary\ntreatment as\n\n$$\nY_{i}(X_{i} = 1, Z_{i}) - Y_{i}(X_{i} = 0, Z_{i})\n$$ {#eq-causal-effect}\n\nAs equation @eq-causal-effect makes clear, a causal effect is the difference\nbetween the value of the outcome we observe under the treatment status $X_{i} = 1$ \nfor unit $i$ and the *counterfactual* and value of the outcome we would have \nobserved under $X_{i} = 0$.\n\n::: a\nIn an ideal world, we could estimate unit-level treatment effects. In practice, \nhowever, since for each unit $i$ we can observe the potential outcome $Y_{i}(X_{i} = 1)$\nor $Y_{i}(X_{i} = 0)$ but it is logically impossible to observe both. As such we \ntypically take as our estimand the treatment effect in some subset of the population. \nFor example, in a Bayesian framework we can express the posterior distribution of the\npopulation average treatment effect as\n\n$$\n\\mathrm{PATE} = \\int\\mathrm{E[Y_{ij}(X_{ij} = 1, Z_{ij})]} - \\mathrm{E[Y_{ij}(X_{ij} = 0, Z_{ij})]}d\\mathrm{Z_{ij}}\n$$ {#eq-bayes-pate}\n\nGiven a set of identifying assumptions, equation @eq-bayes-pate yields an estimate for the \nposterior distribution of the expected change in the outcome if all of the units received \nthe treatment compared to what we would have observed if no unit was treated [@Oganisian2020]. \nWe can represent this process in the form of a DAG as shown in figure @fig-simple-dag, which \nimplies conditional on the set of measured confounders $Z = \\{z_{1},z_{2}, z_{3}\\}$, the \ntreatment assigment is as good as random thus identifying the causal path $X \\longrightarrow Y$.\nThere are several strategies one might take to close the backdoor confounding paths \n$X \\longleftarrow Z \\longrightarrow Y$ including though not necessarily limited to \nrandom assignment of the treatment, propensity-score based methods, and regression \nadjustment. Since the topic of this post is statistical control, however, I limit \nmy focus here to regression adjustment.\n::: \n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n::: {.cell-output-display}\n![Simple DAG for the Effect of X on Y where Z Denotes Observed Confounders and U is an Unobserved Confounder](figs/fig-simple-dag.svg){#fig-simple-dag fig-align='center' width=1536}\n:::\n:::\n\n\n::: a\nAlthough adusting for the set of confounders $Z$ is sufficient to identify the \ntotal effect of $X$ on $Y$, it would be erronous to assume, as many researchers do,\nthis implies any of the confounders in the adjustment set $Z$ are also themselves \ncausally identified. In fact, based on the DAG in figure @fig-simple-dag, it is mathematically \nimpossible to simultaneously identify both $X$ and any variable in the set $Z$ due the biasing \npaths $Z \\longleftarrow U \\longrightarrow Y$ where $U$ is an unobserved confounder. \nThat is, under ideal conditions regression adjustment can be used to estimate causal effects \nin the abscence of random assignment. As a general rule, however, it is unlikely other covariates \nin a regression model may be ascribed a valid causal interpretation without further assumptions \nthat are difficult to defend in practice.\n:::\n\n## Simulation Study\n\nTo further illustrate why the concept of *mutual adjustment* is fundamentally flawed, \nconsider the more complex data generation process depicted by the causal graph in figure \n@fig-dgp-dag. As in the example above, $X$ is some exposure or treatment of interest, \n$Y$ is the outcome, $\\{Z, W, L, J\\}$ is a set of measured confounders, \nand $\\{U, V\\}$ are unobserved confounders.\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n::: {.cell-output-display}\n![DAG for a Hypothetical Data Generation Process](figs/fig-dgp-dag.svg){#fig-dgp-dag fig-align='center' width=1536}\n:::\n:::\n\n\n::: a\nIf our primary objective is to identify the causal path $X \\longrightarrow Y$, this \ncan be acheived by adjusting for the set $\\{Z, L, W, J\\}$ as illustrated in figure @fig-simulation-dgp.[^1] \nAlthough this set is sufficient to identify the path $X \\longrightarrow Y$, it does not identify \nother paths such as $J \\longrightarrow Y$ or $Z \\longrightarrow Y$ which are confounded by \nthe biasing paths $J \\longleftarrow V \\longrightarrow Y$ and $Z \\longleftarrow U \\longrightarrow Y$ \nrespectively. If this all seems abstract, you can simply substitute the letters representing \nnodes in the figure above for theoretical constructs that are more familiar. The important \ntakeaway here is when using statistical adjustment as an empirical strategy, the relationship \nbetween treatment and outcome is the path we care about and the adjustment set is a sacrifice \nwe make on the altar of causal identification.\n:::\n\n[^1]: In this case the adjustment sets necessary to identify the total and direct effect of $X$ on $Y$ are the same, but this may not be the case in other contexts.\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n::: {.cell-output-display}\n![Data Generation Process for the Simulation Study](figs/fig-simulation-dgp.svg){#fig-simulation-dgp fig-align='center' width=1536}\n:::\n:::\n\n\n::: a\nThus far I have levied several bold claims, some of which amount to accusing many \naccomplished scholars of engaging in a practice that effectively amounts to the \npropagation of pseudo-science. Although I have grounded this argument squarely in the \nlogic of causality, some readers may insist upon further illustration of just how wrong\nthe practice of presenting and interpreting coefficients for nuisance parameters\ncan be. After all, if we are able to justify the assumption the unobserved confounder $U$ is \nconditionally independent of $Z$, it is possible to jointly identify the causal paths \n$Z \\longrightarrow Y$ and $X \\longrightarrow Y$ based on the adjustment set\n$\\{L, W, J\\}$. \n\nTo demonstrate the magnitude of the problem, I simulate data based on the theoretical \nprocess depicted in figure @fig-simulation-dgp, manipulating the path $U \\longrightarrow Z$ \nto assess the bias in the estimate for the causal effect of $Z$ on $Y$. I also vary the \ndimensions of the data, considering 2,500, 5,000, and 10,000 observations and repeat the \nsimulation 500 times per cell, resulting in 3,000 unique datasets. To provide a breif overview of \nthe simulation design, I begin by drawing fixed values for the unobserved confounders $U$ and $V$ \nat 1.0 and 0.5, respectively. The path coefficients $\\gamma_{k}$ for the dependencies between nodes \nare then drawn from a normal distribution such that $\\gamma_{k} \\sim \\mathrm{Normal(0.5, 0.5)}$ for \n$k \\in \\{1, \\dots, K\\}$. The treatment propensity $\\theta$ for the exposure $X$ and the measured \nconfounders are then each a function of their respective ancestor nodes and random noise \n$\\sigma \\sim \\mathrm{Normal(0, 0.1)}$ as follows\n\n$$\n\\begin{align}\nZ_{i} &\\sim \\gamma_{5}W + \\gamma_{6}L_{i} + \\delta U + \\sigma_{Z}\\\\\nL_{i} &\\sim \\gamma_{3}U_{i} + \\gamma_{4}V_{i} + \\sigma_{L}\\\\\nJ_{i}&\\sim \\gamma_{2}V + \\sigma_{J}\\\\\nW_{i} &\\sim \\gamma_{1}U + \\sigma_{W}\\\\\n\\end{align}\n$$ and for the observed treatment\n\n$$\n\\begin{align}\nX_{i} &\\sim \\mathrm{Bernoulli}(\\theta_{i})\\\\\n\\theta_{i} &= \\mathrm{logit}^{-1}(\\gamma_{7}Z_{i} + \\gamma_{8} W_{i} + \\gamma_{9} J_{i} + \\gamma_{10} L_{i} + \\sigma_{X})\\\\\n\\end{align}\n$$\n::: \n\nwhere the $\\delta U$ in the equation for $Z$ represents the experimental manipulation and $\\sigma_{X} \\sim \\mathrm{Normal(0, 0.01)}$.\n\n::: a\nFinally, the outcome $Y_{i}$ is a function of a fixed intercept $\\alpha$, coefficients for\neach parameter $\\beta_{k}$, the unobserved confounders $U$ and $V$, and a random noise term $\\sigma$ \nas expressed in equation @eq-outcome-dgp\n::: \n\n$$\nY_{i} \\sim \\alpha + \\beta_{1}X_{i} + \\beta_{2}Z_{i} + \\beta_{3}L_{i} + \\beta_{4}W_{i} + \\beta_{5}J_{i} + V + U + \\sigma\n$${#eq-outcome-dgp}\n\nwhere $\\alpha = 0.50$ and $\\sigma~\\sim~\\mathrm{Normal}(0, 0.5)$. \n\n::: a\nUnder this assumed DGP, the true value of $Z$ should be recoverable if and only if the unobserved \nconfounder $U$ is conditionally independent of $Z$. On the other hand, all of the covariates in the\nadjustment set $\\{J, W, L\\}$ may be expected to exhibit severe bias due to unobserved confounding. \nCode for simulating the data in R and Python is shown below.\n:::\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulated DGP Based on the DAG in Figure 2\nsim_dag_data <- function(N, a, b, cond, conf) {\n  # Coefficients for ancestor nodes\n  g <- rnorm(10, 0.5, 0.5)\n    \n  # Unobserved Confounders U and V\n  V <- conf[1] + rnorm(N, 0, 0.1)\n  U <- conf[2] + rnorm(N, 0, 0.1)\n    \n  # Measured Confounders {Z, L, J, W}\n  W <- g[1] * U + rnorm(N, 0, 0.1)\n  J <- g[2] * V + rnorm(N, 0, 0.1)\n  L <- g[3] * U + g[4] * V + rnorm(N, 0, 0.1)\n  Z <- g[5] * W + (U * cond) + g[6] * L + rnorm(N, 0, 0.1)\n    \n  # Treatment X\n  logit_theta <- g[7] * Z + g[8] * W + g[9] * J + g[10] * L + rnorm(N, 0, 0.01)\n  theta <- exp(logit_theta)/(1 + exp(logit_theta))\n  X <- rbinom(N, size = 1, prob = theta)\n    \n  # Linear Predictor Data Generation Process\n  mu <- b[1] * X + b[2] * Z + b[3] * L + b[4] * J + b[5] * W\n    \n  # Observed Response Y\n  Y <- a + mu + U + V + rnorm(N, 0, 0.2)\n    \n  # Combine everything into a data frame\n  out <- data.table(X, Z, L, J, W, Y)\n\n  # Return just the data table\n  return(out)\n} \n```\n:::\n\n\n### Python\n\n```python\n# Required dependencies for the DAG data\nfrom numpy.random import normal as rnorm\nfrom numpy.random import uniform as runif\nfrom numpy.random import binomial as rbinom\nfrom numpy import exp, repeat\nfrom polars import DataFrame, Series\n\n# Inverse Logistic Transformation\ndef inv_logit(logit_theta):\n  prob = exp(logit_theta)/(1 + exp(logit_theta))\n  return prob\n\n# Simulated DGP Based on the DAG in Figure 1\ndef sim_dag_data(N, a, b, cond, conf):\n  # Coefficients for ancestor nodes\n  g = rnorm(0.5, 0.5, 10)\n    \n  # Unobserved Confounders U and V\n  V = conf[\"V\"] + rnorm(0, 0.1, N)\n  U = conf[\"U\"] + rnorm(0, 0.1, N)\n    \n  # Measured Confounders {Z, L, J, W}\n  W = g[0] * U + rnorm(0, 0.1, N)\n  J = g[1] * V + rnorm(0, 0.1, N)\n  L = g[2] * U + g[3] * V + rnorm(0, 0.1, N)\n  Z = g[4] * W + (U * cond) + g[5] * L + rnorm(0, 0.1, N)\n    \n  # Treatment X\n  logit_theta = g[6] * Z + g[7] * W + g[8] * J + g[9] * L + rnorm(0, 0.01, N)\n  theta = inv_logit(logit_theta)\n  X = rbinom(n = 1, p = theta, size = N)\n    \n  # Linear Predictor Data Generation Process\n  mu = b[\"X\"] * X + b[\"Z\"] * Z + b[\"L\"] * L + b[\"J\"] * J + b[\"W\"] * W\n    \n  # Observed Response Y\n  Y = a + mu + U + V + rnorm(0, 0.2, N)\n  \n  # Store the Condition\n  cond = Series(repeat(cond, N))\n\n  # Combine everything into a data frame\n  out = {\"X\": X, \"Z\": Z, \"L\": L, \"J\": J, \"W\": W, \"Y\": Y, \"Condition\": cond}\n  return DataFrame(out)\n```\n:::\n\n::: a\nSince this is a Bayesian statistics blog, we'll specify the model and estimate the parameters using \nHamiltonian Monte Carlo (HMC), though the concept outlined above holds regardless of inferential \nframework and taking a Bayesian approach may in fact be constraining how severe the bias in the\nparameter estimates can be.[^2] Our model here takes the following form\n::: \n\n[^2]: One could also simply specify a frequentist prior here by assuming that all parameter values between $-\\infty$ and $\\infty$ are equally likely.\n\n$$\n\\begin{align}\ny_{i} &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n\\mu &= \\alpha + X_{n}\\beta_{k} + \\sigma\\\\\n\\text{with priors}\\\\\n\\alpha &\\sim \\mathrm{Normal}(\\bar{y}, ~2\\cdot\\sigma_{y})\\\\\n\\beta_{k} &\\sim \\mathrm{Normal}\\left(0, ~2 \\cdot \\frac{\\sigma_{y}}{\\sigma_{x_{k}}}\\right) ~ \\mathrm{for~} k \\in \\{1,\\dots,K\\}\\\\\n\\sigma &\\sim \\mathrm{Exponential}\\left(\\frac{1}{\\sigma_{y}}\\right)\n\\end{align}\n$${#eq-model-formula}\n\nwhere priors in equation @eq-model-formula are specified based on the scaling approach outlined in @Gelman2021 \nand thus automatically adjusted to be weakly informative for each of the simulated datasets. We can specify the model \nin Stan as follows\n\n```stan\ndata {\n  int<lower=1> N; // Observations\n  int<lower=1> K; // Population-Level Coefficients\n  vector[N] Y; // Response\n  matrix[N, K] P; // Design Matrix for the Fixed Effects\n  vector[K] truth; // True Values of the Coefficients\n}\n\ntransformed data {\n  // Centering the Design Matrix\n  matrix[N, K] X;  // Centered version of P\n  vector[K] X_means; // Column Means of P\n  vector[K] X_sd; // Column SDs of P\n  for (i in 1:K) {\n    X[, i] = P[, i] - mean(P[, i]);\n    X_means[i] = mean(P[, i]);\n    X_sd[i] = sd(P[, i]);\n  }\n\n  // Data for the Intercept priors\n  real mu_alpha;\n  real sigma_alpha;\n  mu_alpha = mean(Y);\n  sigma_alpha = 2 * sd(Y);\n\n  // Data for the Coefficient priors\n  vector[K] beta_sd;\n  beta_sd = 2 * (sd(Y)/X_sd);\n\n  // Prior for the residual sd\n  real sigma_prior;\n  sigma_prior = 1/sd(Y);\n}\n\nparameters {\n  real alpha; // Intercept for the Centered Predictors\n  vector[K] beta; // Regression Coefficients\n  real<lower = 0> sigma; // Residual Noise\n}\n\nmodel {\n  // Likelihood\n  target += normal_id_glm_lpdf(Y | X, alpha, beta, sigma);\n\n  // Priors on the parameters\n  target += exponential_lpdf(sigma | sigma_prior);\n  target += normal_lpdf(alpha | mu_alpha, sigma_alpha);\n  target += normal_lpdf(beta | 0, beta_sd);\n}\n\ngenerated quantities {\n  // Recover the Non-Centered Intercept and Coefficients\n  real Intercept = alpha - dot_product(beta, X_means);\n\n  // Bias in the Parameter Estimates\n  vector[K] bias;\n  bias = truth - beta;\n}\n```\n\n::: a\nAlthough the models themselves only take a second or two to fit, repeating the process\n3,000 times quickly becomes computationally burdensome. It is possible to side-step this\nproblem by defining functions to prepare the data and fit models in parallel via the \n`{furrr}` package and `{cmdstanr}` as shown below. The `make_stan_data` function\nprepares the simulated datasets to be passed to Stan while the `sim_bayes` function\nfits the models and returns the necessary `draws_df` object containing the estimated\nparameters and generated quantities. This approach reduces the wall time for the simulations\nfrom 1.5 hours to about thirty minutes.\n\nFor each model, I run four Markov chains in parallel for 2,000 total iterations per chain with the \nfirst 1,000 for each chain discarded after the initial warm-up adaptation stage. The total run time \nfor the simulations under Stan version 2.3.0 is approximately 35 minutes on a Windows 10 desktop computer \nwith an 12-core Ryzen 9 5900X CPU and 128GB of DDR4 memory.\n:::\n\n::: panel-tabset\n\n### Functions\n```r\n# Function for Building the Data to Pass to the Stan Model\nmake_stan_data <- function(data, truth, ...) {\n  \n  # Predictor Positions\n  x_pos <- grep(\"[J-X]|Z\", colnames(data))\n  \n  # Extract the predictor matrix\n  P <- data[, ..x_pos]\n  \n  # Extract the response\n  Y <- data$Y\n  \n  # Prepare the data for use with Stan\n  stan_data <- list(\n    N = nrow(P), # Observations\n    K = ncol(P), # K Predictors\n    Y = Y, # Response\n    P = P, # Design Matrix\n    truth = truth\n  )\n  \n  # Return the list of data\n  return(stan_data)\n}\n\n# Function for fitting the Stan Models in Parallel\nsim_bayes <- function(stan_model, \n                      stan_data,\n                      sampling, \n                      warmup, \n                      seed, \n                      chains,\n                      ...) {\n  \n  # Set the initial number of draws to 0\n  min_draws <- 0\n  \n  # Repeat the run if any of the chains stop unexpectedly\n  while (min_draws < (sampling * chains)) {\n    \n    # Fit the Stan Model\n    sim_fit <- stan_model$sample(\n      data = stan_data,\n      sig_figs = 5,\n      parallel_chains = chains,\n      iter_warmup = warmup,\n      iter_sampling = sampling,\n      max_treedepth = 12,\n      adapt_delta = 0.9,\n      seed = seed,\n      show_messages = FALSE,\n      ...\n    )\n    \n    # Update the check\n    min_draws <- posterior::ndraws(sim_fit$draws())\n  }\n  \n  # Calculate a summary of the draws\n  sim_draws <- sim_fit$draws(format = \"draws_df\")\n  \n  # Return the data frame of draws\n  return(sim_draws)\n}\n```\n\n### Data Simulation\n```r\n# Load the necessary libraries\npacman::p_load(\n  \"tidyverse\",\n  \"data.table\",\n  \"cmdstanr\",\n  \"posterior\",\n  \"furrr\",\n  install = FALSE # Set this to true to install missing packages\n)\n\n# Set the initial rng seed\nset.seed(2023)\n\n# True Values for the Coefficients\nbetas <- c(-0.5, 0.00, 0.5, 0.00, 0.5)\nnames(betas) <- c(\"X\", \"Z\", \"L\", \"J\", \"W\")\n\n# Simulate 3,000 datasets of varying dimensions \nsim_data_df <- expand.grid(\n  N = c(2.5e3, 5e3, 10e3),\n  delta = c(FALSE, TRUE),\n  rep = 1:500\n) %>%\n  # Nest the data by columns\n  nest(sim_pars = c(delta, N)) %>%\n  # Simulate the datasets\n  mutate(sim_data = map(\n    .x = sim_pars,\n    ~ map2(\n      .x = .x$delta,\n      .y = .x$N,\n      ~ sim_dag_data(\n        N = .y, \n        a = 0.5, \n        b = betas, \n        cond = .x,\n        conf = c(0.5, 1.0)\n      ))\n  )) %>%\n  # Unnest the data dimensions\n  unnest(cols = c(sim_pars, sim_data))\n```\n\n### Estimation\n\n```r\n# Generate the data to pass to the stan models\nsims_stan_data <- map(\n  .x = sim_data_df$sim_data,\n  ~ make_stan_data(\n    data = .x,\n    truth = betas\n  ))\n\n# Compile the Stan model\nsim_mod <- cmdstan_model(\"bayes-linreg.stan\")\n\n# Parallel computation via furrr\nplan(tweak(multisession, workers = 3))\n\n# Fit models and add the draws to the simulation data frame\nsim_draws_df <- sim_data_df %>% \n  mutate(sim_draws = future_map(\n    .x = sims_stan_data,\n    .f = ~ sim_bayes(\n      stan_data = .x,\n      stan_model = sim_mod,\n      sampling = 1e3,\n      warmup = 1e3,\n      seed = 12345,\n      chains = 4,\n      refresh = 1e3\n    ),\n    .options = furrr_options(\n      scheduling = 1,\n      seed = TRUE,\n      prefix = \"prefix\"\n    ),\n    .progress = TRUE\n  ))\n\n# Back from the future\nplan(sequential)\n```\n\n### Post-Estimation\n```r\n# Parallel computation via furrr\nplan(tweak(multisession, workers = 8))\n\n# Summarize the nested draws_df objects\nsim_draws_df <- sim_draws_df %>% \n  mutate(sim_draws_summ = future_map(\n    .x = sim_draws,\n    .f = ~ summarise_draws(.x, default_summary_measures()),\n    .options = furrr_options(\n      scheduling = 1,\n      seed = TRUE,\n      prefix = \"prefix\"\n    ),\n    .progress = TRUE\n  ))\n\n# Back from the future\nplan(sequential)\n\n# Extract and combine the posterior draws\nsim_draws_combined <- sim_draws_df %>% \n  # Subset the needed columns\n  select(rep:N, sim_draws_summ) %>% \n  # Unnest the draws, this requires about 25GB of memory\n  unnest(cols = sim_draws_summ) %>% \n  # Filter estimates and bias for X and Z\n  filter(str_detect(variable, \"b.*[1-5]\")) %>% \n  # Generate identifiers and labels\n  mutate(\n    # Coefficient or Bias Identifier\n    type = if_else(str_detect(variable, \"beta\"), \"Coefficient\", \"Bias\"),\n    # Manipulated condition\n    condition = if_else(delta == TRUE, \"Z Confounded\", \"Z Unconfounded\"),\n    # Parameter names\n    param = case_when(\n      variable %in% c(\"bias[1]\", \"beta[1]\") ~ \"X\",\n      variable %in% c(\"bias[2]\", \"beta[2]\") ~ \"Z\",\n      variable %in% c(\"bias[3]\", \"beta[3]\") ~ \"L\",\n      variable %in% c(\"bias[4]\", \"beta[4]\") ~ \"J\",\n      variable %in% c(\"bias[5]\", \"beta[5]\") ~ \"W\"\n    ),\n    # True Parameter values\n    truth = case_when(\n      param %in% c(\"L\", \"W\") ~ 0.5,\n      param %in% c(\"Z\", \"J\") ~ 0.0,\n      param == \"X\" ~ -0.5\n    ))\n```\n:::\n\n::: a\nSince most researchers, at least in practice, adhere to dichotomous decision thresholds\nand test against point nulls, I start here by assessing the coverage rates for the\n90% credible intervals for the posterior distribution of each parameter. As table \n@tbl-coverage-probs illustrates, the credible intervals capture the true parameter\nvalue at or near nominal rates for $X$ and for $Z$ when $U$ is conditionally independent.\nIf, however, $U$ and $Z$ are correlated the 90% credible intervals will virtually always \nfail to capture the true parameter value. Furthermore, as expected, recovery rates for the parameters \n$W$, $J$, and $L$ are generally abysmal under either condition and tend to decline as $n \\longrightarrow \\infty$. \nThe practical implication here is that if $U$ and $Z$ are in fact correlated, and we proceed to present and\ninterpret $Z$ as if its respective coefficient is somehow meaningful, we will almost always be \nwrong. The picture is even more dire for the other parameters which are unlikely to have any\nmeaningful interpretation under either scenario.\n:::\n\n\n::: {#tbl-coverage-probs .cell .tbl-cap-location-top layout-align=\"center\" tbl-cap='Coverage Probabilities for 90% Bayesian Credible Intervals by Parameter'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 14ptpx; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; \" colspan=\"3\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Z Confounded</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; \" colspan=\"3\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Z Unconfounded</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:center;\"> 2,500 </th>\n   <th style=\"text-align:center;\"> 5,000 </th>\n   <th style=\"text-align:center;\"> 10,000 </th>\n   <th style=\"text-align:center;\"> 2,500 </th>\n   <th style=\"text-align:center;\"> 5,000 </th>\n   <th style=\"text-align:center;\"> 10,000 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X </td>\n   <td style=\"text-align:center;\"> 0.89 </td>\n   <td style=\"text-align:center;\"> 0.89 </td>\n   <td style=\"text-align:center;\"> 0.91 </td>\n   <td style=\"text-align:center;\"> 0.89 </td>\n   <td style=\"text-align:center;\"> 0.90 </td>\n   <td style=\"text-align:center;\"> 0.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Z </td>\n   <td style=\"text-align:center;\"> 0.01 </td>\n   <td style=\"text-align:center;\"> 0.01 </td>\n   <td style=\"text-align:center;\"> 0.00 </td>\n   <td style=\"text-align:center;\"> 0.93 </td>\n   <td style=\"text-align:center;\"> 0.91 </td>\n   <td style=\"text-align:center;\"> 0.90 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> L </td>\n   <td style=\"text-align:center;\"> 0.16 </td>\n   <td style=\"text-align:center;\"> 0.11 </td>\n   <td style=\"text-align:center;\"> 0.07 </td>\n   <td style=\"text-align:center;\"> 0.06 </td>\n   <td style=\"text-align:center;\"> 0.05 </td>\n   <td style=\"text-align:center;\"> 0.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> J </td>\n   <td style=\"text-align:center;\"> 0.13 </td>\n   <td style=\"text-align:center;\"> 0.07 </td>\n   <td style=\"text-align:center;\"> 0.06 </td>\n   <td style=\"text-align:center;\"> 0.11 </td>\n   <td style=\"text-align:center;\"> 0.08 </td>\n   <td style=\"text-align:center;\"> 0.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> W </td>\n   <td style=\"text-align:center;\"> 0.29 </td>\n   <td style=\"text-align:center;\"> 0.21 </td>\n   <td style=\"text-align:center;\"> 0.15 </td>\n   <td style=\"text-align:center;\"> 0.14 </td>\n   <td style=\"text-align:center;\"> 0.13 </td>\n   <td style=\"text-align:center;\"> 0.09 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n::: a\nJust how wrong are these parameter estimates likely to be? Here I move beyond \nsimple error probabilities and look more closely at errors of *magnitude*.[^3] Conditional\non having already committed a Type I error and concluding that our credible interval contains the true\nparameter value when it in fact does not,[^4] figure @fig-type-m-error shows the average root mean \nsquare error for each parameter in which the 90% credible interval fails to capture the true value.\nWe see that on average the magnitude of bias in covariates can be quite large and in the case both \n$Z$ and $X$ are jointly identified, the magnitude of bias in the parameters $L$ and $W$ is on average \n**worse** than when $Z$ and $U$ are correlated. Nor does this bias decrease as $n \\longrightarrow \\infty$, \nunderscoring the reality that \"big data\" is not a substitute for experimental design or causal reasoning.\n:::\n\n[^3]: If it isn't obvious at this point, I am leaning very heavily on the Bernstein-von Mises theorem here.\n\n[^4]: Or in the frequentist case, whatever weird and tortured interpretation of a confidence interval you prefer because Neyman was a fundamentalist who refused to consider uncertainty in terms of beliefs.\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n::: {.cell-output-display}\n![Average Parameter Bias in Models that Do not Recover the True Value](figs/fig-type-m-error.svg){#fig-type-m-error fig-align='center' width=1536}\n:::\n:::\n\n\n::: a\nAs table @tbl-coverage-probs illustrated, if we present and interpret $L$ or $J$, or $W$ as if they are total effect\nestimates, we will on average be wrong 88.5%, 91.6%, and 78.4% of the time, respectively. Likewise, if\nthe assumption that $U$ and $Z$ are conditionally independent fails, our chance of being wrong is 99.6%.\nThis makes the practice of reporting and interpreting nuisance parameters in tables--often filled with\nlargely meaningless astrisks--deeply concerning because, at least in principle, the goal of scientific\ninquiry and research more broadly is *to be less wrong*.\n\nThe important takeaway here is anything other than $X$ or whatever your main feature of interest \nhappens to be, is a **nuisance parameter** and generally should not be presented or interpreted as \nif its respective coefficient or marginal effect estimate has some causal meaning because the \nchances your research design is capable of identifying every variable included in a model are \nvirtually guaranteed to be zero. These estimates most certainly are not, as a collegue recently suggested, \n\"a part of your contribution to existing research.\" In fact, since presenting and interpreting hopelessly \nconfounded coefficient estimates or marginal effects encourages those reading your work to do the same, \nthose who persist in committing the table 2 fallacy or encourage others to do so are actively harming \nthe pursuit of knowledge in their field by peddling what is in no uncertain terms pseudo-science.\n\nFinally, for those wondering whether this applies to analyses whose aims are primarily descriptive or \nexploratory in nature the answer is \"yes.\" Since identifying relevant confounders is a process that \nmust be informed and justified by theory, the need to adjust for potentially confounding factors itself \nimplies causal aims as does any reporting or reliance on arbitrary decision rules such as \n\"statistical significance\" as a means of establishing whether a relationship exists [@Wysocki2022]. \nIndeed, outside of a causal framework p-values in the social sciences have no meaning or valid \ninterpretation and thus the practice of reporting, at a minimum, \"sign and significance\" has always \nbeen and continues to be misguided.\n::: \n\n## Conclusion\n\nThe overarching and unnecessarily long-winded point of this post is that applied researchers should\nfocus on the feature of the world they care about. If one is interested in a relationship between the\nimplementation of gender quotas and women's representation in government, for example, one should simply\nfocus on estimating the impact of quota adoption on the representation of women in government rather than\nwasting words opining about political corruption, electoral systems, or some other nuisance parameter.[^5] \n\n[^5]: In many ways this recommendation echoes that made by @Huenermund2020\n\n::: a\nOne might attempt to object on the grounds incompetent reviewers often demand otherwise principled \nresearchers engage in poor statistical practices but this is easily solved by placing all relevant tables \nin the appendix and adhereing to the common sense guideline that one should never present in a table what \ncould be communicated in a graph. This post and the recommendations herein are broadly applicable in both \nexperimental and observational contexts. The problem remains prevelant in both top tier journals and those \nwith less prestige.\n\nOf course, encouraging researchers to improve their own practices is only half the battle because bad habits \nand logical fallacies are learned behaviors oweing to the reality that graduate-level statistics in the \nsocial sciences is often taught entirely independent of any meaningful causal foundation. Students are \ninstructed to interpret everything with the justification being that \"they need experience in interpreting \ncoefficients/marginal effects.\" Yet, this has the unintended consequence of instilling in them that\nsuch an approach should be taken in their own work and they then go onto to teach their future students \nthose same poor practices. Before long, this results in entire fields in which presumed knowledge rests\nupon castles of sand and hinders scientific progress.\n::: \n\n# References",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}