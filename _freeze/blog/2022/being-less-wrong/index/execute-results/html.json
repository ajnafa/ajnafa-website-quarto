{
  "hash": "bd2626a51566583349ef2137ea881c40",
  "result": {
    "markdown": "---\ntitle: An Introduction to Being Less Wrong\nsubtitle: Bayesian Hypothesis Testing and Model Averaged Marginal Effects for the Social Sciences\npagetitle: An Introduction to Being Less Wrong\ndate: 2022-12-31\ndescription: \"This post provides an introduction to Bayesian Model Averaging and Model Averaged Marginal Effects with Stan, `{brms}`, and `{marginaleffects}`\"\ncategories: [R, Bayesian Statistics, Statistics, Marginal Effects, Stan, Logit, Model Averaging]\nlicense: All content in this post is made available for public use under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License.\nimage: figs/fig-model-averaged-ame-margins.svg\ntoc-location: left\npage-layout: full\ncode-fold: true\n\n# Execution options\nexecute: \n  freeze: auto\n---\n\n\n\n\n\n# Introduction\n\nGeorge E. P. Box famously described the problem of model development in statistics and related fields by noting, \n*all models are wrong, but some are useful* [-@Box1976]. That is, all models are necessarily simplifications \nof a much more complex reality and statistics is by no means an algorithmic truth generating process because no \nmodel can ever be true in the pure sense of the word. Yet, this at times underappreciated reality does not render \nall models useless but rather implies the goal of those who practice statistics in its various forms is to \ndevelop and identify models that are useful in a never ending quest to be less wrong. In this post I introduce an \napproach to this task that has been the subject of my own dissertation work on the use of Bayesian Model Averaging \n(BMA) to account for uncertainty in model specification when estimating average marginal effects in non-linear \nregression models. \n\n::: a \nI outline here what I term a Bayesian Model Averaged Marginal Effect (BMAME) in the context of logistic regression \nmodels in political science and illustrate how to obtain BMAMEs in a straightforward manner using the \n`{marginaleffects}` R package [@ArelBundock2022a], which thanks to Vincent Arel-Bundock's assistance supports model averaged or\nstacked average marginal effects for any model fit with the Stan interface `{brms}` [@Burkner2017; @Burkner2018]. \nI also provide a more general demonstration in Stan that makes the method easily applicable in Python and \nhigh dimensional settings. Although I focus here on models that include \nonly population-level effects, recent feature additions to `{marginaleffects}` now make it possible to obtain\nBMAMEs from more complex hierarchical models and I will cover such applications in a subsequent post.\n::: \n\n# Bayesian Model Averaging in Political Science\n\nAlthough Bayesian Model Averaging was introduced to political science more than two decades ago \nby @Bartels1997, cases of its use in applied work remain rare and are largely confined to the topic \nof political methodology [@Juhl2019; @Cranmer2015; @Montgomery2010].[^1] This presents something of a \nproblem given the numerous studies demonstrating the dangers of tossing a large number of likely correlated \npredictors into a regression model [@Achen2005; @Clarke2005; @Clarke2009; @Montgomery2018]; the reality that \nif we wish to adjudicate between two or more competing theories, comparing coefficients in a single model is \ngenerally insufficient to accomplish such a task [@Imai2011; @Clarke2007; @Hollenbach2020]; and the difficulty \nof assessing what we truly known about political phenomenon that results from an obsession with \"statistical \nsignificance\" and precise answers to poorly defined questions [@Ward2010; @Schrodt2014; @Lundberg2021]. \n\n[^1]: For a recent application of BMA in the context of instrument selection see @Rozenas2019.\n\n::: a \nIn contrast to the advice of @Montgomery2010 who suggest \"BMA is best used as a subsequent robustness \ncheck to show that our inferences are not overly sensitive to plausible variations in model specification\" \n(266), I argue here that model averaging can and should be used as far more than a robustness check if for\nno other reason than because a model-wise mixture distribution for our paramter of intertest is almost \ncertainly more informative than a point estimate from a single model specification. Indeed, BMA provides\none of the few natural and intuitive ways of resolving some of the issues outlined in the preceding \nparagraph, particularly as it pertains to the evaluation of non-nested theoretical models, while also\nexhibiting a lower false positive rate than alternative approaches [@Pluemper2018]. This section provides \na brief explanation of BMA and its connection to model averaged marginal effects before \nproceeding to a discussion of how the procedure is implemented.\n::: \n\n## Bayesian Model Comparison and Model Averaging\n\nConsider a simple case in which we have a set of $\\mathcal{M}$ models, each of which characterizes a \npossible representation of some unknown data generation process. Although it remains common practice \nin applied political science to select a single model based on a largely arbitrary fit statistic \n(i.e., AIC, $\\chi^{2}$, or whatever people are getting a p-value from this week), such an approach is \nproblematic since it ignores the uncertainty inherent in the process of model specification. Rather \nthan selecting a single model and consequently placing an implicit zero prior on \n**every other possible model specification you could have reasonably considered but did not**, Bayesian Model \nAveraging and its analogues provide a means of averaging across several possible, potentially \nnon-nested, model specifications and accounting for the uncertainty associated with doing so.\n\n::: a \nImagine we wish to adjudicate between two competing theoretical models in the set of candidate models \n$\\mathcal{M}$. The posterior odds of model $i$ relative to an alternative $k$ can be expressed as\n:::\n\n$$\n\\underbrace{\\frac{\\Pr(\\mathcal{M}_{i} \\, | \\, y)}{\\Pr(\\mathcal{M}_{k} \\, | \\, y)}}_{\\mathrm{Posterior~Odds}} = \\underbrace{\\frac{p(y \\, | \\, \\mathcal{M}_{i})}{p(y \\, | \\, \\mathcal{M}_{k})}}_{\\mathrm{Bayes~Factor}} \\times \\underbrace{\\frac{\\pi(\\mathcal{M}_{i})}{\\pi(\\mathcal{M}_{k})}}_{\\mathrm{Model~Prior}}\n$$ {#eq-posterior-odds} \n\nwhere $\\pi(\\mathcal{M})$ is the prior probability of model $i$ over model $k$ and $p(y \\,|\\, \\mathcal{M})$ \nis the marginal likelihood of the observed data under each of the candidate models such that\n\n$$\n\\underbrace{p(y \\,|\\, \\mathcal{M})}_{\\mathrm{Marginal~Likelihood}} = \\int\\underbrace{p(y \\,|\\, \\theta,\\, \\mathcal{M})}_{\\mathrm{Likelihood}} \\, \\underbrace{\\pi(\\theta \\,|\\, \\mathcal{M})}_{\\mathrm{Prior}}d\\theta\n$$ {#eq-marginal-likelihood}\n\n::: a \nGiven our prior assumptions about how likely the observed data are to have arisen from each model in the set \n$\\mathcal{M}$, the result of equation @eq-posterior-odds is the posterior odds of model $i$ compared to model \n$k$ and thus captures the relative probability that model $\\mathcal{M_{i}}$ represents the true data generation \nprocess compared to the plausible alternative $\\mathcal{M_{k}}$. We can easily extend this example to the setting \nin which $\\mathcal{M_{k}}$ is a set of plausible competing models of size $k > 2$, in which case the posterior \nmodel probability of the $i^{th}$ model relative to each of the alternatives $k$ is\n:::\n\n$$\n\\Pr(\\mathcal{M}_{i} \\,|\\, y) = \\frac{p(y \\, | \\, \\mathcal{M}_{i}) \\, \\cdot \\, \\pi(\\mathcal{M}_{i})}{\\displaystyle\\sum^{\\mathcal{m}}_{k=1} p(y \\, | \\, \\mathcal{M}_{k}) \\, \\cdot \\, \\pi(\\mathcal{M}_{k})}\n$$ {#eq-posterior-probability} \n\nApplying equation @eq-posterior-probability to each model in the set under consideration \nyields a vector of relative posterior model probabilities of length $m$ which may be used as either posterior \nprobability weights as in the case of Bayesian Model Averaging or for the estimation of posterior inclusion \nprobabilities.\n\n::: a \nIn the context of model averaging, we can take draws from the posterior predictive distribution of each model \ncontaining a given parameter of interest proportional to its posterior probability weight obtained from equation \n@eq-posterior-probability. This yields a model-wise mixture representing a weighted average of the posterior \npredictive distribution for some parameter such as a regression coefficient for a linear model or as I outline \nbelow, an average marginal effect or probability contrast for models that employ a non-linear link function. \n::: \n\n## Bayesian Model Averaged Marginal Effects\n\nAverage marginal effects are ubquitous in the social sciences though, as \n[Andrew Heiss nicely illustrates](https://www.andrewheiss.com/blog/2022/05/20/marginalia/), the term average \nmarginal effect is often used ambiguously and in the context of more complex models, the terms marginal and\nconditional [tend to be a source of additional confusion](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/). \nIn the case of a simple population-level linear model with a Gaussian likelihood and identity link function, \nthe model averaged marginal effect is equivalent to the posterior distribution of the model averaged \ncoefficient $\\beta$ which can be expressed as\n\n$$\n\\mathbb{E}(\\beta \\, | \\, y) = \\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} \\, | \\, y) \\cdot \\mathbb{E}(\\beta\\, |\\, \\mathcal{M}_{i}, \\, y) \\quad \\forall \\quad i \\in \\{1, 2,\\dots, m\\}\n$$ {#eq-bma-population} \n\nwhere $\\Pr(\\mathcal{M}_{i} ~ | ~ y)$ represent the posterior model probability and \n$\\mathbb{E}(\\beta ~|~ \\mathcal{M}_{i}, ~ y)$ is the expected value of some parameter $\\beta$ \nconditional on the observed data and some model specification for each model $i$ in the set of \nmodels under consideration $m$ [@Montgomery2010].\n\n::: a\nFor probability models such as logit, and other commonly used formulations for modeling\ndiscrete responses, this simple closed form solution does not exist nor is there any meaningful\nway to interpret the coefficients of these models as efffect size estimates--a reality that unfortunately \nremains quite widely misunderstood [@Mood2009; @Daniel2020]. As a general principle, **any model more\ncomplex than a simple linear regression can only be meaningfully interpreted in terms of the predictions\nit generates** [@Long2018], which in the case of logistic regression simply means applying the inverse \nlogistic function to obtain predicted probabilities which we can then use to estimate contrasts or average \nmarginal effects [@Norton2019]. Letting $\\mathrm{Y}$ represent a binary outcome, $\\mathrm{X}$ a continuous \nexposure or treatment of interest, and $\\mathrm{Z}$ a matrix of measured confounders, we can express the \naverage marginal effect as\n:::\n\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\frac{\\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij} + h, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij}, \\mathrm{Z}_{ij}]d\\mathrm{Z}}{h} \\\\\n$$ {#eq-logit-continuous-ame} \n\nwhich provided a sufficiently small value of $h$, yields a reasonable approximation of the partial derivative \nand provides a posterior distribution for the average effect of an instantaneous change in $\\mathrm{X}$ on the\nprobability scale similar to what one obtains in linear regression.\n\n::: a\nSimilarly, for a binary treatment of interest we apply the Bayesian g-formula to obtain the average marginal \neffect of $\\mathrm{X}$ on the probability of $\\mathrm{Y}$ as shown in equation @eq-logit-binary-ame.\n:::\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n$$ {#eq-logit-binary-ame}\n\nEquations @eq-logit-continuous-ame and @eq-logit-binary-ame make clear we are averaging over the confounder \ndistribution $\\mathrm{Z}$ rather than holding it constant at some fixed value [@Oganisian2020; @Keil2017]. \nThis point is an important one because in binary outcome models such as logit and probit, holding $\\mathrm{Z}$ \nconstant at some fixed value such as the mean corresponds to **an entirely different estimand** [@Hanmer2012].\nThe AME is a population-averaged quantity, corresponding to the population average treatment effect and these\ntwo quantities can produce very different results under fairly general conditions because **they do not answer \nthe same question**.\n\n::: a\nFrom here it is straightforward to obtain a model averaged marginal effect estimate for a discrete outcome model\nsuch as logit. Given marginal predictions for each model $k$ in the set $m$, we can apply equation @eq-logit-binary-bmame \nto obtain the posterior distribution of average marginal effects for each model.\n:::\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n&= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\n\\end{align}\n$$ {#eq-logit-binary-bmame} \n\nThis yields a model-wise mixture distribution representing the weighted average of the marginal effects\nestimates and has broad applicability to scenarios common in the social sciences, industry, and any \nother setting where a need arises to account for uncertainty in model specification.\n\n## Limitations and Caveats\n\nWhile traditional marginal likelihood-based Bayesian model averaging is a powerful and principled \ntechnique for dealing with uncertainty in the process of model specification and selection, \nit is not without limitations and caveats of its own, some of which bear emphasizing before \nproceeding further.\n\n### Marginal Likelihood and Computational Uncertainty\n\nFirst, for all but the simplest of models attempting to derive the integral in equation \n@eq-marginal-likelihood analytically proves to be computationally intractable and we must \ninstead rely on algorithmic approximations such as bridge sampling [@Gronau2017; @Gelman1998; @Wang2020].\nWhile the bridge sampling approximation tends to perform well compared to common alternatives,\nestimates of the marginal likelihood $p(y \\,|\\, \\mathcal{M})$ may be highly variable upon \nrepeated runs of the algorithm [@Schad2022]. This problem tends to arise in more\ncomplex hierarchical models with multiple varying effects and, perhaps unsurprisingly, suggests\nthat by relying on approximations we introduce an additional source of computational uncertainty \nthat may need to be accounted for.\n\n::: a\nFor the purposes of BMAMEs, I have elsewhere proposed addressing this additional source of uncertainty \nand incorporating it into the estimates by executing the bridge sampling algorithm $S$ times for each \nmodel to obtain a distribution of possible values for the approximate log marginal likelihood. Given a \nvector of estimates for $p(y ~|~ \\mathcal{M})$ of length $s$ for each model $k \\in \\{1,2,\\dots, m\\}$, \nwe can apply equation @eq-posterior-probability to obtain an $s \\times m$ matrix of posterior model \nprobabilities. It is then straightforward to extend equation @eq-logit-binary-bmame to average over the \ndistribution of posterior probability weights rather than relying on single estimate or point estimate \nas shown in equation @eq-logit-bmame-reps below.[^2]\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &=\\frac{1}{S}\\sum_{s=1}^{S} \\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z}\\right] \\\\\n&= \\frac{1}{S}\\sum_{s=1}^{S}\\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\\right]\n\\end{align}\n$$ {#eq-logit-bmame-reps} \n\nWhether this approach is necessary depends largely on how variable the algorithm is on repeated runs\nand it may simply be adequate to instead take the median or mean of the marginal likelihood estimates for\neach model. Although it is beyond the scope of this post, one could also use exhaustive leave-future-out \ncross-validation which is equivalent to the marginal likelihood given a logarithmic scoring rule \n[@Fong2020; @Buerkner2020], though this approach tends to be extremely expensive in terms of computation.\n:::\n\n[^2]: It is entirely possible this is wrong and does not actualy make sense so if someone has a better idea or suggestions for improvements I would love to hear them.\n\n### Bayes Factors and Prior Sensitivity\n\nThe second caveat with respect to obtaining BMAMEs stems from a common criticism of Bayes Factors. Although\nas the amount of available data $n \\longrightarrow \\infty$, the likelihood will tend to dominate the prior \nin Bayesian estimation, this is not necessarily the case in terms of inference. Indeed, from equations \n@eq-marginal-likelihood and @eq-posterior-probability one can quite clearly see the the posterior probability \n$\\Pr(\\mathcal{M} ~| ~y)$ depends on the prior probability of the parameters, $\\pi(\\theta \\,|\\, \\mathcal{M})$ \nin equation @eq-marginal-likelihood, and the respective prior probability $\\pi(\\mathcal{M})$ of each model \nin the set under consideration. Since our substantive conclusions may be sensitive to either of these prior \nspecifications, these assumptions should be checked to verify the results do not vary wildly under reasonable \nalternatives. \n\n::: a\nPerhaps more importantly, due to their dependence on Bayes Factors, posterior probability weights require at\nleast moderatlely informative priors on each and every parameter in a model and it is by this point common \nknowledge that specifying a flat or \"uninformative\" prior such as $\\beta \\sim \\mathrm{Uniform(-\\infty, \\infty)}$ \nor even an excessively vague one tends to bias Bayes Factors--and by extension values that depend on \nthem--violently in favor of the null model.[^3] This is a feature of applied Bayesian inference rather than a\nbug and **if you make stupid assumptions, you will arrive at stupid conclusions**. The solution to this problem \nis to think carefully about the universe of possible effect sizes you might observe for a given parameter, \nassign reasonable priors that constrain the parameter space, and verify the results are robust to \nalternative distributional assumptions.\n\n[^3]: There is no such thing as an \"uninformative prior\"; the concept itself is not mathematically defined.\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Likeihoods and Priors\n\nThe degree to which a prior can be considered informative is heavily dependent on the likelihood and can often \nonly be understood in that context [@Gelman2017].\n:::\n\n### Stacking, and the Open-$\\mathcal{M}$ Problem\n\nFinally, traditional BMA is flawed in the sense that its validity rests upon the closed $\\mathcal{M}$ \nassumption implicit in equation @eq-posterior-probability. In the open $\\mathcal{M}$ setting where the \ntrue model is not among those in the set of candidate models under consideration, as will generally \nbe the case in any real world application, estimated effects based on posterior probability \nweights are likely to be biased [@Hollenbach2020; @Yao2018]. This problem arises from the fact that \nmarginal-likelihood based posterior probability weights for each model $m \\in \\mathcal{M}$ correspond to \nthe probability a given model captures the true data generation process and in most settings this is unlikely \nto ever be the case since, by definition, *all models are wrong*. \n\n::: a\nIn the open $\\mathcal{M}$ setting, traditional BMA will still assign each model some probability but since by \ndefinition $\\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} ~|~ y) = 1$, these weights no longer have a valid \ninterpretation in terms of the posterior probability a given model is true. Although this does not render marginal \nlikelihood based model averaging useless and it may often make more sense to think about model specification and \nselection as a probabilistic process aimed at identifying the most likely model conditional on the set of models \nunder consideration [@Hinne2020], it is advisable to assess whether our substantive conclusions are sensitive to \npotential violations of the closed $\\mathcal{M}$ assumption. This once again underscores the fact that statistics is \nnot a mechanistic truth generating process and *there is no magic*. It also underscores the need to think in more \nlocal terms--rather than asking *is this model true* we generally want to know *the probability $\\mathrm{X}$ \nbelongs to the true data generation process*.[^4]\n\n[^4]: As @Pluemper2018 demonstrate, BMA performs quite well in terms of this latter objective.\n\nAn alternative but still properly Bayesian approach aimed at addressing these potential flaws in traditional BMA \nis cross-validation based stacking [@Yao2018]. In contrast to traditional BMA, stacking \nand pseudo-BMA weights are constructed based on the relative expectation of the posterior predictive \ndensity [@Hollenbach2020; @Yao2018]. These approaches to estimating model weights do not rely upon the closed \n$\\mathcal{M}$ assumption, and thus provide a way of either avoiding it altogether or relaxing it as a robustness \ncheck on the posterior probability weights, albeit while answering a fundamentally different question more appropriate \nfor an open $\\mathcal{M}$ world. Since the overall approach to calculating model averaged AMEs does not change\nand we are merely substituting the posterior probability weights in equation @eq-logit-binary-bmame with\nstacking or pseudo-BMA weights, I direct readers to @Yao2018 for a formal discussion of these approaches.\n::: \n\n# Female Combatants and Post-Conflict Rebel Parties\n\nTo illustrate the utility of BMAMEs in political science, I draw on an example adapted from the second chapter of\nmy dissertation in which I examine how the active participation of women in rebel groups during wartime shapes \nthe political calculus of former combatant groups at war's end. In short, I expect rebel groups where women \nparticipate in combat roles during wartime to be *more likely* to form political parties and participate in \npost-conflict elections. \n\n## Data\n\nTo identify the universe of post-conflict successor parties to former combatant groups I rely on the Civil \nWar Successor Party (CWSP) dataset which includes all civil wars in the UCDP Armed Conflicts Dataset (UCDP) \nthat terminated between 1970 and 2015 [@Daly2020]. Since the primary distinction between political parties \nand other civil society organizations such as interest groups lies in the latter's participation in elections \nand fielding candidates for political office and a party that does not compete in elections is no party at all, \nI operationalize our dependent variable, *Rebel to Party Transition*, as a binary event which takes a value of \n1 if a rebel group both forms a political party and directly participates in a country's first post-conflict \nelections and 0 otherwise. The resulting sample consists of 112 observations for 108 unique rebel groups \nacross 56 countries, approximately 66% of which formed parties and participated in their country's first \npost-conflict election.\n\n::: a\nFollowing @Wood2017, I define female combatants as members of a rebel group who are \"armed and participate in \norganized combat activities on behalf of a rebel organization\" (38). I code instances of female participation \nin rebel groups by mapping version 1.4 of the Women in Armed Rebellion Data (WARD) to the CWSP and construct \ntwo versions of the main predictor of interest based on the best estimate for the presence of female combatants \nin a given rebel group, one of which includes cases in which women served only as suicide bombers while \nthe other codes these groups as not having any women in combat positions. Each of these variables takes a value \nof 1 if women participated in combat in a given rebel group and 0 otherwise. Among the rebel groups in our \nanalysis, female combatants have been active in some capacity in 52.67% of those who were involved in civil wars \nthat have terminated since 1970 and 50.89% featured women in non-suicide bombing roles.\n\nI also adjust for several additional factors that may otherwise confound the estimate of the effect of \nfemale participation in combat on rebel successor party transitions. First, insurgent groups that attempt \nto perform functions typically characteristic of the state during wartime may be more likely to transition \nto political parties following the end of a conflict since, in doing so, they both develop organizational \ncapacity and if successful, demonstrate that they are capable of governing to their future base of support. \nMoreover, as rebel groups build organizational capacity and accumulate institutional knowledge, their ability \nto sustain a successful insurgency and recruit combatants grows as well. To adjust for rebel organizational \ncapacity I specify a Bayesian multidimensional item response model to construct a latent scale capturing the \ndegree to which rebel groups engaged in functions of the state during wartime based on twenty-six items from \nQuasi-State Institutions dataset [@Albert2022].\n\nSecond, I adjust for baseline characteristics of each rebel group that previous research suggests influences \nboth where women rebel and post-conflict party transitions. Drawing on data from @Braithwaite2019, I construct \na series of dichotomous indicator variables for whether a rebel group is affiliated with an existing political \nparty [@Manning2016], nationalist and left-wing or communist ideological origins [@Wood2017], and whether a \ngroup has ties to a specific ethnic community since this may provide advantages in terms of mobilizing \ncombatants and voters along specific cleavages. I also construct two additional indictators to capture \nwhether the United Nations intervened in the conflict and whether a given rebel group was located in Africa.\n\nWhile it is, of course, impossible in the context of an observational setting such as this one to isolate\nevery possible source of confounding, nor is it generally advisable to attempt to do so, I focus on \nthese baseline covariates since existing research provides a strong theoretical basis to suspect they \nimpact the relationship of interest.[^5] For the purposes of this illustration, I assume this set of covariates\nis sufficient to satisfy strong ignorability but such an assumption is likely unreasonable and \nthe particular analysis here may ultimately best be taken as descriptive in nature.\n:::\n\n[^5]: Those familiar with the post-conflict rebel parties literature in political science are likely rolling their eyes at this sentence because in its present state it is not clear we know much of anything about causal relationships.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the required libraries\npacman::p_load(\n  \"tidyverse\", \n  \"arrow\",\n  \"data.table\",\n  \"brms\", \n  \"furrr\", \n  \"cmdstanr\",\n  \"patchwork\",\n  \"ggdist\",\n  \"marginaleffects\",\n  install = FALSE\n)\n\n# Read in the pre-processed data\nmodel_df <- read_parquet(\"data/model_data.gz.parquet\")\n```\n:::\n\n\n\n## Model and Priors\n\nTo estimate the effect of women's participation in combat on post-conflict rebel party transitions for \nthis example, I specify a simple logit model as shown in equation @eq-logit-model. The bernoulli \nprobability $\\mu$ of a rebel group transitioning to a political party at war's end is modeled as a \nfunction of the population-level intercept $\\alpha$ and a vector of coefficients $\\beta_{k}$ corresponding \nto the matrix of inputs $X_{n}$.\n\n$$\n\\begin{align}\ny_{i} &\\sim \\mathrm{Bernoulli}(\\mu)\\\\\n\\mathrm{logit}(\\mu) &= \\alpha + X_{n}\\beta_{k}\\\\\n\\text{with priors}\\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 1)\\\\\n\\beta_{k} &\\sim \\mathrm{Normal}(0, 0.75)\n\\end{align}\n$$ {#eq-logit-model}\n\n::: a\nSince we model $\\mu$ via a logit link function, our priors on the intercept $\\alpha$ and \ncoefficients $\\beta_{k}$ should correspond to the expected change in the log odds of the response \nfor each predictor. Some may find the process of prior elicitation in logistic regression models \ndifficult since reasoning on the logit scale may seem unintuitive. As figure @fig-logit-scale\nillustrates, the the log odds of the response for models with that employ a logit link function \nis effectively bounded between approximately $-6$ and $6$. Values in excess of $|5|$ generally should not \noccur unless your data exhibits near complete or quasi-complete separation.\n:::\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n## Simulate logit probabilities\nlogit_scale <- tibble(\n  x = seq(-6.5, 6.5, 0.0001),\n  y = brms::inv_logit_scaled(x)\n)\n\n## Plot the logit scale\nggplot(logit_scale, aes(x = x, y = y)) +\n  ## Add a line geom\n  geom_line(size = 3, color = \"firebrick\") +\n  ## Add labels\n  labs(x = \"Log Odds\", y = \"Probability\") +\n  ## Adjust the x axis scale breaks\n  scale_x_continuous(breaks = seq(-6, 6, 1)) +\n  ## Adjust the y axis scale breaks\n  scale_y_continuous(breaks = seq(0, 1, 0.1))\n```\n\n::: {.cell-output-display}\n![Scale of the Logit Link](figs/fig-logit-scale.svg){#fig-logit-scale fig-align='center' width=1344}\n:::\n:::\n\n::: a\nFramed in terms of effect sizes, a change of $\\pm 3$ in the log odds of the response relative to the baseline spans half the logit scale in either direction. In the social sciences where data is largely random noise and effect sizes are typically small to moderate at best and while the road to hell is paved with *rules of thumb*, we can generally assume *a priori* that 1) any true effect of a given parameter $\\beta$ in the population is likely modest in magnitude and 2) values closer zero are more likely than those further away. From these relatively straightforward assumptions, we can arrive at a prior for the coefficients that follows a normal distribution with mean 0 and scale $0.75$ as shown in equation @eq-logit-model. In practical terms, this assumes the universe possible effect sizes for each predictor is normally distributed in the range of approximately $-1.5$ and $1.5$ on the logit scale. \n\nFor the population-level intercept $\\alpha$ I assign a slightly more diffuse normal prior mean 0 and scale 1 or $\\alpha \\sim \\text{Normal}(0,~1)$ which reflects a slightly greater degree of uncertainty about the baseline probability but is still moderately informative and constrains the bulk of the prior density in the range of realistic values. It is important to note here that as shown in Stan model below, we are specifying a prior on the intercept for the centered inputs rather than on the original scale. To more explicitly communicate our prior assumptions, we can use the tools for visualizing uncertainty provided in the `{ggdist}` package as shown in figure @fig-logit-priors below [@Kay2022].\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulate the prior probabilities\nlogit_priors <- tibble(\n  alpha_prior = rnorm(10e5, 0, 1),\n  beta_prior = rnorm(10e5, 0, 0.75)\n) %>% \n  ## Pivot the simulated data to long form\n  pivot_longer(everything())\n\n# Set parameter names for the facets----\nlogit_prior_labels <- as_labeller(\n  x = c(\n    \"beta_prior\" = \"bolditalic(beta[k] * phantom(.) * phantom() %~% phantom() * phantom(.) * Normal(0*list(,) * phantom(.) * 0.75))\",\n    \"alpha_prior\" = \"bolditalic(alpha[k] * phantom(.) * phantom() %~% phantom() * phantom(.) * Normal(0*list(,) * phantom(.) * 1))\"\n  ), \n  default = label_parsed\n)\n\n# Plot the prior distributions\nlogit_prior_plots <- logit_priors %>% \n  # Initiate the ggplot object\n  ggplot(aes(x = value, fill = name, shape = name)) +\n  # Facet by the prior distributions\n  facet_wrap(~ name, scales = \"free\", labeller = logit_prior_labels) +\n  # Add a half eye geom for the prior distributions\n  stat_halfeye(\n    aes(slab_alpha = stat(pdf)),\n    .width = c(.68, .95),\n    fill_type = \"gradient\",\n    show.legend = FALSE\n  ) +\n  # Manually set the fill colors\n  scale_fill_manual(values = c(\"#208820\", \"#0060B0\")) +\n  # Manually set the shapes\n  scale_shape_manual(values = c(22, 24)) +\n  # Adjust the breaks on the y axis\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +\n  # Add labels to the plot\n  labs(\n    y = \"Prior Probability Density\",\n    x = latex2exp::TeX(r'(Log $\\Pr(\\theta_{prior})$)', bold = T)\n  )\n\n# Plot back transformed probabilities\nprob_trans_plots <- logit_priors %>% \n  # Initiate the ggplot object\n  ggplot(aes(fill = name, shape = name)) +\n  # Facet by the prior distributions\n  facet_wrap(~ name, scales = \"free\", labeller = logit_prior_labels) +\n  # Add a half eye geom for the prior distributions\n  stat_halfeye(\n    aes(x = plogis(value), slab_alpha = stat(pdf)),\n    .width = c(.68, .95),\n    fill_type = \"gradient\",\n    show.legend = FALSE\n  ) +\n  # Manually set the fill colors\n  scale_fill_manual(values = c(\"#208820\", \"#0060B0\")) +\n  # Manually set the shapes\n  scale_shape_manual(values = c(22, 24)) +\n  # Add labels to the plot\n  labs(\n    y = \"Prior Probability Density\",\n    title = \"\",\n    x = latex2exp::TeX(r'($\\Pr(\\theta_{prior})$)', bold = TRUE)\n  ) +\n  # Adjust the breaks on the x axis\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 8))\n\n## Print the combined plot\nlogit_prior_plots/prob_trans_plots\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Priors for the Logit Model Parameters](figs/Prior_Probs_S2.png){#fig-logit-priors fig-align='center' width=2700}\n:::\n:::\n\n\n\n::: a\nSince the model here is computationally quite efficient--on average, each model takes approximately 2.5 \nseconds to converge with 30,000 post-warmup draws--and in the interest of completeness, I search the \nfull parameter space which consists of $2^{k}$ possible model specifications. Although it is possible \nto do this in this context, researchers will often instead need to place constrain the number of possible \nmodel configurations, generally by specifying an implicit prior model probability of 1 for certain \nparameters [@Montgomery2010; @Hinne2020]. Although `{brms}` provides an easy to use R interface to Stan \nthat makes fitting a wide variety of Bayesian regression models easier than ever before and requires no \ndirect knowledge of Stan, it is limited for my purposes here by the fact that it requires recompiling \nthe underlying Stan model for each fit. This process ends up being extremely inefficient for a large \nnumber of model configurations since each model takes three times as long to compile as it does to fit.\n\nTo circumvent this issue, I implement the model directly in Stan as shown below which is designed to be \ndirectly compatible with brms and implments the bayesian g-formula for the average marginal effect of \nwomen's participation in combat on the probability of a rebel-to-party transition during the estimation \nprocess in the generated quanitites block. As I previously demonstrated in \n[this post](../statistical-adjustment-interpretation/index.qmd), it is then possible to compile the \nStan model once and use the same program for each dataset using `{cmdstanr}` directly. Further \ncomputational gains can be obtained by using the `{furrr}` package to fit multiple models in \nparallel and I specify a series of functions to streamline this process as shown below.\n:::\n\n::: panel-tabset\n\n### g-formula\n```stan\n// Bayesian Logit Model with Optional Bootstrapped AMEs via the Bayesian g-formula\ndata {\n  int<lower=1> N; // N Observations\n  array[N] int Y; // Response Variable\n  int<lower=1> K; // Number of Population-Level Effects\n  matrix[N, K] X; // Population-level Design Matrix\n  int prior_only; // Should the likelihood be ignored?\n  int treat_pos; // Data for the bootstrapped AMEs\n}\n\ntransformed data {\n\n  int Kc = K - 1;\n  matrix[N, Kc] Xc; // centered version of X without an intercept\n  vector[Kc] means_X; // column means of X before centering\n\n  for (i in 2 : K) {\n    means_X[i - 1] = mean(X[ : , i]);\n    Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n  }\n  \n  // Bootstrap Probabilities\n  vector[N] boot_probs = rep_vector(1.0/N, N);\n  \n  // Matrices for the bootstrapped predictions\n  matrix[N, Kc] Xa; \n  matrix[N, Kc] Xb;\n    \n  // Potential Outcome Y(X = 1, Z)\n  Xa = X[, 2:K];\n  Xa[, treat_pos - 1] = ones_vector(N);\n    \n  // Potential Outcome Y(X = 0, Z)\n  Xb = X[, 2:K];\n  Xb[, treat_pos - 1] = zeros_vector(N);\n}\n\nparameters {\n  vector[Kc] b; // Regression Coefficients\n  real Intercept; // Centered Intercept\n}\n\ntransformed parameters {\n  real lprior = 0; // Prior contributions to the log posterior\n  lprior += normal_lpdf(b | 0, 0.75);\n  lprior += normal_lpdf(Intercept | 0, 1);\n}\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | Xc, Intercept, b);\n  }\n\n  // priors including constants\n  target += lprior;\n}\n\ngenerated quantities {\n\n  // Actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n\n  // Additionally sample draws from priors\n  array[Kc] real prior_b = normal_rng(0, 0.75);\n  real prior_Intercept = normal_rng(0, 1);\n  \n  // Row index to be sampled for bootstrap\n  int row_i;\n    \n  // Calculate Average Marginal Effect in the Bootstrapped sample\n  real AME = 0;\n  array[N] real Y_X1; // Potential Outcome Y(X = 1, Z)\n  array[N] real Y_X0; // Potential Outcome Y(X = 0, Z)\n    \n  for (n in 1:N) {\n    // Sample the Baseline Covariates\n    row_i = categorical_rng(boot_probs);\n      \n    // Sample Y(x) where x = 1 and x = 0\n    Y_X1[n] = bernoulli_logit_rng(b_Intercept + Xa[row_i] * b);\n    Y_X0[n] = bernoulli_logit_rng(b_Intercept + Xb[row_i] * b);\n      \n    // Add Contribution of the ith Observation to the Bootstrapped AME\n    AME = AME + (Y_X1[n] - Y_X0[n])/N;\n  }\n    \n  // Take the mean of the posterior expectations\n  real EYX1 = mean(Y_X1); // E[Y | X = 1, Z]\n  real EYX0 = mean(Y_X0); // E[Y | X = 0, Z]\n}\n```\n\n### Reference\n\n```stan\n// Bayesian Logit Model for Models without the Treatment Variable\ndata {\n  int<lower=1> N; // total number of observations\n  array[N] int Y; // response variable\n  int<lower=1> K; // number of population-level effects\n  matrix[N, K] X; // population-level design matrix\n  int prior_only; // should the likelihood be ignored?\n}\n\ntransformed data {\n\n  int Kc = K - 1;\n  matrix[N, Kc] Xc; // centered version of X without an intercept\n  vector[Kc] means_X; // column means of X before centering\n\n  for (i in 2 : K) {\n    means_X[i - 1] = mean(X[ : , i]);\n    Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n  }\n}\n\nparameters {\n  vector[Kc] b; // Regression Coefficients\n  real Intercept; // Centered Intercept\n}\n\ntransformed parameters {\n  real lprior = 0; // Prior contributions to the log posterior\n  lprior += normal_lpdf(b | 0, 0.75);\n  lprior += normal_lpdf(Intercept | 0, 1);\n}\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | Xc, Intercept, b);\n  }\n\n  // priors including constants\n  target += lprior;\n}\n\ngenerated quantities {\n\n  // Actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n\n  // Additionally sample draws from priors\n  array[Kc] real prior_b = normal_rng(0, 0.75);\n  real prior_Intercept = normal_rng(0, 1);\n}\n```\n\n### Functions\n\n```r\n#' A Function for building combinations of formulas for model averaging\n#' \n#' @importFrom glue glue_collapse\n#' @importFrom purrr map_chr\n#' @importFrom stringr str_replace\n#'\n#' @param terms A named list of terms to create all unique combinations of.\n#' See usage example for direction on how to specify each term\n#' \n#' @param resp A string specifying the name of the response vector\n#' \n#' @param base_terms An optional argument indicating additional terms to\n#' include in all formulas. This can be used to place implicit constraints on \n#' the parameter space to reduce computational burden\n#' \n#' @param ... Reserved for future development but currently\n#' unused\n#'\n#' @return A character vector of model formulas containing all unique \n#' combinations of the elements in terms\n#' \n#' @export bma_formulas\n#' \n#' @examples\n#' # Specify the input for the resp argument\n#' response <- \"fun\"\n#' \n#' # Specify the inputs for the terms argument\n#' covariates <- list(\n#'   age = c(\"\", \"age\"),\n#'   sex = c(\"\", \"sex\"),\n#'   rock = c(\"\", \"rock\")\n#'   )\n#'   \n#' # Build the formulas\n#' model_forms <- bma_formulas(terms = covariates, resp = response)\n#' \n\nbma_formulas <- function(terms, resp, base_terms = NULL, ...) {\n  \n  ## Validate the formula arguments\n  stopifnot(exprs = {\n    is.list(terms)\n    is.character(resp)\n  })\n  \n  ## Make a list of all unique combinations of lhs ~ rhs \n  formulas <- expand.grid(terms, stringsAsFactors = FALSE)\n  \n  ## Initialize a list to store the formulas in\n  out <- list()\n  \n  ## Build a list of formulas for each pair formulas\n  for (i in 1:nrow(formulas)) {\n    out[[(i)]] <- glue_collapse(\n      formulas[i, !is.na(formulas[i, ])], \n      sep = \" + \"\n    )\n  }\n  \n  # Paste the response term\n  out <- paste(resp, \"~\", out)\n  \n  # If base_terms is non-null, add additional terms to all formulas\n  if (!is.null(base_terms)) {\n    out <- paste(out, base_terms, sep = \" + \")\n  }\n  \n  out <- map_chr(\n    .x = out, \n    ~ str_replace(.x, \"character\\\\(0\\\\)\", \"1\")\n    )\n  \n  return(out)\n}\n\n#' Fitting Models in Parallel with brms via cmdstanr\n#'\n#' @param formula A `brmsformula` object for the model.\n#' \n#' @param treat A string argument containing the name of the treatment\n#' or a regular expression to match against the columns in `data`.\n#' \n#' @param prior A `brmsprior` object containing the priors for the model.\n#' \n#' @param data A data frame containing the response and inputs named in\n#' `formula`.\n#' \n#' @param cores The maximum number of MCMC chains to run in parallel.\n#' \n#' @param chains The number of Markov chains to run.\n#' \n#' @param warmup The number of warmup iterations to run per chain.\n#' \n#' @param sampling The number of post-warmup iterations to run per chain.\n#' \n#' @param stan_models The compiled Stan program(s) to be passed to \n#' `cmdstanr::sample`.\n#' \n#' @param save_file The file path where the brmsfit object should be saved.\n#' \n#' @param ... Additional arguments passed to `brms::brm`\n#'\n#' @return An object of class `brmsfit`\n#' \n#' @export cmdstan_logit\n#'\ncmdstan_logit <- function(formula,\n                          treat,\n                          prior,\n                          data,\n                          cores,\n                          chains,\n                          warmup,\n                          sampling,\n                          stan_models,\n                          save_file,\n                          ...) {\n  # Check if the file already exists\n  file_check <- file.exists(save_file)\n  \n  if (isTRUE(file_check)) {\n    # Read in the file if it already exists\n    out <- readr::read_rds(file = save_file)\n  } else {\n    \n    # Total iterations for brms\n    niter <- sampling + warmup\n    \n    # Initialize an empty brmsfit object\n    brms_obj <- brm(\n      formula = formula,\n      family = brmsfamily(\"bernoulli\", link = \"logit\"),\n      prior = prior,\n      data = data,\n      cores = cores,\n      chains = chains,\n      iter = niter,\n      warmup = warmup,\n      empty = TRUE,\n      seed = 123456,\n      sample_prior = \"yes\",\n      ...\n    )\n    \n    # Get the data for the stan model\n    stan_data <- standata(brms_obj)\n    \n    # Check whether the model contains the treatment\n    stan_data$treat_pos <- .match_treat_args(stan_data, treat)\n    \n    # Select the Stan model to use based on the data\n    if (!is.null(stan_data$treat_pos)) {\n      cmdstan_model <- stan_models[[\"gformula\"]]\n    } else {\n      cmdstan_model <- stan_models[[\"reference\"]]\n    }\n    \n    # Fit the brms model\n    out <- .fit_cmdstan_to_brms(cmdstan_model, brms_obj, stan_data, \n                                cores, chains, warmup, sampling)\n    \n    # Write the brmsfit object to an rds file\n    readr::write_rds(out, file = save_file, \"xz\", compression = 9L)\n    \n  }\n  \n  # Return the model object\n  return(out)\n}\n\n# Function for detecting the treatment vector\n.match_treat_args <- function(stan_data, treat) {\n  \n  # Check if any of the columns in X match the treatment\n  check_treat <- grepl(treat, colnames(stan_data$X))\n  \n  # Get the position of the treatment if present\n  if (any(check_treat)) {\n    treat_pos <- which(check_treat)\n  } else {\n    treat_pos <- NULL\n  }\n  \n  return(treat_pos)\n}\n\n# Function to fit the models with cmdstanr and store them in the empty brmsfit\n.fit_cmdstan_to_brms <- function(cmdstan_model,\n                                 brms_obj,\n                                 stan_data,\n                                 cores,\n                                 chains,\n                                 warmup,\n                                 sampling) {\n  \n  # Set the initial number of draws to 0\n  min_draws <- 0\n  \n  # Repeat the run if any of the chains stop unexpectedly\n  while (min_draws < (sampling * chains)) {\n    \n    # Fit the Stan Model\n    fit <- cmdstan_model$sample(\n      data = stan_data,\n      seed = 123456,\n      refresh = 1000,\n      sig_figs = 5,\n      parallel_chains = cores,\n      chains = chains,\n      iter_warmup = warmup,\n      iter_sampling = sampling,\n      max_treedepth = 13\n    )\n    \n    # Update the check\n    min_draws <- posterior::ndraws(fit$draws())\n  }\n  \n  # Convert the environment to a stanfit object and add it to the brmsfit\n  brms_obj$fit <- rstan::read_stan_csv(fit$output_files())\n  \n  # Rename the parameters\n  brms_obj <- brms::rename_pars(brms_obj)\n  \n  # Store the expectations inside the brmsfit object\n  if (!is.null(stan_data$treat_pos)) {\n    \n    # Extract the expectations and contrasts\n    brms_obj$predictions <- fit$draws(\n      variables = c(\"EYX0\", \"EYX1\", \"AME\"), \n      format = \"draws_df\"\n    )\n  }\n  \n  # Store run times\n  brms_obj$times <- fit$time()\n  \n  # Return the brmsfit object\n  return(brms_obj)\n}\n```\n:::\n\n::: a\nWe can then generate a formula for each of the $2^{k}$ model combinations, specify priors, and compile the Stan \nprograms to be passed to `cmdstan_logit` as shown below. I estimate each of the 384 possible candidate models,\nrunning six parallel markov chains for iterations 8,000 each and discarding the first 3,000 after the initial warmup \nadaptation stage.[^6] This leaves 30,000 post-warmup draws per model for post-estimation inference. While this is well \nin excess of what is required for reliable estimation, the large number of samples is necessary to ensure \nstability of the bridge sampling approximation for the marginal likelihood [@Schad2022]. Fitting three models in \nparallel via `{furrr}`, estimation and approximate leave one out cross validation take less than 10 minutes in total \nto complete. Oweing in part to the fact I run the algorithm 250 times per model, however, the run time to estimate the \nmarginal likelihood via bridge sampling takes roughly 9 hours in total.\n:::\n\n[^6]: Estimation is performed under Stan version 2.30.1 on a desktop computer with a Ryzen 9 5900X CPU and 128GB of DDR4 memory.\n\n::: panel-tabset\n\n### Preparation\n\n```r\n# Define the list of predictors, parameter space is 2^k\nmodel_terms <- list(\n  femrebels = c(\"femrebels\", \"femrebels_exs\", NA),\n  left_ideol = c(\"left_ideol\", NA),\n  nationalist = c(\"nationalist\", NA),\n  un_intervention = c(\"un_intervention\", NA),\n  party_affil = c(\"party_affil\", NA),\n  ethnic_group = c(\"ethnic_group\", NA),\n  africa = c(\"africa\", NA),\n  qsi = c(\"qsi_pol + qsi_econ + qsi_social\", NA)\n)\n\n# Build the model formulas, results in 383 combinations\nmodel_forms <- bma_formulas(terms = model_terms, resp = \"elect_party\") %>% \n  map(.x = ., ~ bf(.x))\n\n# Specify priors for the model parameters\nlogit_priors <- prior(normal(0, 0.75), class = b) +\n  prior(normal(0, 1), class = Intercept)\n\n# Names of the files to write the models to\nfits_files <- str_c(\n  main_priors_dir,\n  \"Party_Transition_Logit_Main_\",\n  seq_along(model_forms),\n  \".rds\"\n)\n\n# Compile the Stan models\nbayes_logit_mods <- list(\n  \"reference\" = cmdstan_model(\n    stan_file = str_c(\n      main_priors_dir, \n      \"Party_Transition_Logit_Main.stan\"\n      ), \n    force_recompile = FALSE\n  ),\n  \"gformula\" = cmdstan_model(\n    stan_file = str_c(\n      main_priors_dir, \n      \"Party_Transition_Logit_Main_gformula.stan\"\n      ), \n    force_recompile = FALSE\n  )\n)\n```\n\n### Estimation\n\n```r\n# Planning for the future\nplan(tweak(multisession, workers = 3))\n\n# Fit the models in parallel via {future}\nbayes_model_fits <- future_map2(\n  .x = model_forms,\n  .y = fits_files,\n  .f = ~ cmdstan_logit(\n    formula = .x,\n    prior = logit_priors,\n    treat = \"femrebels|femrebels_exs\",\n    data = model_df,\n    cores = 6,\n    chains = 6,\n    warmup = 3e3,\n    sampling = 5e3,\n    stan_models = bayes_logit_mods,\n    save_file = .y,\n    control = list(max_treedepth = 13),\n    save_pars = save_pars(all = TRUE)\n  ),\n  .options = furrr_options(\n    scheduling = 1,\n    seed = TRUE,\n    prefix = \"prefix\"\n  ),\n  .progress = TRUE\n) # Takes about ~5 minutes in total\n```\n\n### Post-Estimation\n\n```r\n# Add PSIS LOO CV Criteria\nbayes_model_fits <- future_map2(\n  .x = bayes_model_fits,\n  .y = fits_files,\n  .f = ~ add_criterion(\n    .x,\n    criterion = \"loo\",\n    cores = 4,\n    file = stringr::str_remove(.y, \".rds\")\n  ),\n  .options = furrr_options(\n    scheduling = 1,\n    seed = TRUE,\n    prefix = \"prefix\"\n  ),\n  .progress = TRUE\n) # Takes about ~4 minutes in total\n\n# Add log marginal likelihood----\nbayes_model_fits <- future_map(\n  .x = bayes_model_fits,\n  .f = ~ add_criterion(\n    .x,\n    criterion = \"marglik\",\n    max_iter = 5e3,\n    repetitions = 250,\n    cores = 8,\n    silent = TRUE\n  ),\n  .options = furrr_options(\n    scheduling = 1,\n    seed = TRUE,\n    prefix = \"prefix\"\n  ),\n  .progress = TRUE\n) # Takes about ~9.5 hours in total\n\n# Back from the future\nplan(sequential)\n```\n::: \n\n## Hypothesis Testing\n\nIn a Bayesian framework, model comparison is formalized by means of posterior model probabilities as shown in \nequation @eq-posterior-probability. Given $m$ possible candidate models and letting $i$ denote those containing \nthe parameter of interest and $k$ those that do not, we can obtain posterior *inclusion* and *exclusion* \nprobabilities for a given parameter. The ratio of these two probabilities shown in equation @eq-inclusion-prob \nis an inclusion Bayes Factor and represents the relative posterior odds of models containing the parameter of \ninterest compared to all of the candidate models that do not.\n\n$$\n\\mathrm{BF_{Inclusion}}~=~\\frac{\\sum_{i=1}\\Pr(\\mathcal{M}_{i} \\, | \\, y)}{\\sum_{k=1}\\Pr(\\mathcal{M}_{k} \\, | \\, y)}\n$$ {#eq-inclusion-prob}\n\n::: a\nIn terms of hypothesis testing, posterior inclusion probabilities provide the *epistemologically correct* way \nto answer the questions most social science studies are concerned with--given our modeling assumptions, how\nlikely are those models containing the treatment $\\mathrm{X}$ compared to those that do not. In other words, a posterior\ninclusion probability provides a continuous measure of evidence we can use to evaluate how probable it is our\ntreatment or intervention of interest belongs to the true data generation process [@Hinne2020].[^7] \n\nAlthough my interest here lies primarily in using the posterior model probabilities to obtain a model-wise mixture \ndistribution of the substantive effect estimates, I breifly illustrate how one can formally test a hypothesis \nin a Bayesian framework and obtain a vague answer to a precise question. This may be useful in a sequential testing\napproach where we first wish to evaluate whether an effect exists before preceding to interpretation though one\nmay also simply assume effects exist and focus on estimating their magnitude instead.\n\nFor the purposes of this illustration, I compare those models containing each of the two measures of women's \nparticipation in combat during civil wars to each of the possible model specifications that does not include \neither of the two measures. Although by convention, it is common to take the prior model probabilities to be\nequally likely--in effect an implicit uniform prior--this is far from the only possible approach and in many\ncases, particularly when adjudicating between competing theories, it may make more sense to weight the hypotheses\naccording to some external source such as existing research on the subject, meta-analyses, etc. For simplicity\nhere, I assume each of the models is equally likely a priori and obtain posterior probabilities and inclusion \nBayes Factors for the parameters of interest as shown below.[^8]\n:::\n\n[^7]: One might contrast this with frequentist p-values in the NHST framework common in the social sciences which answer the question *under the simplifying assumption the null hypothesis is true, what is the probability of observing a result more extreme than the one we did?* Although still widely taught, this provides a very precise answer to a question few researchers are interested in and rejecting the strawman null that the effect of $\\mathrm{X}$ is exactly equal to zero--something no respectable researcher actually believes--tells you nothing about your vaguely defined alternative [@Gross2014; @Gill1999].\n\n[^8]: This is only in part because I can't figure out the correct `brms::do_call` syntax to pass both a list of marglik objects *and* model priors to `bridgesampling::post_prob`.\n\n::: panel-tabset\n\n### Preparation\n\n```r\n# Extract the parameters of interest in each model\nmodel_params <- map(\n  bayes_model_fits, \n  ~ variables(.x) %>% \n    str_subset(., \"^b_\")\n)\n\n# Build a reference data frame with the positions of the predictors\nparams_pos_df <- tibble(\n  # Model Names\n  model = str_c(\"M\", 1:length(model_params)),\n  # Female Rebels (Including Suicide Bombers)\n  femrebels_pos = map_lgl(\n    model_params,\n    ~ str_which(.x, \"b_femrebels$\") %>% \n      length() == 1\n  ),\n  # Female Rebels (Excluding Suicide Bombers)\n  femrebelsexs_pos = map_lgl(\n    model_params, \n    ~ str_which(.x, \"b_femrebels_exs\") %>% \n      length() == 1\n  ),\n  # No Female Rebels Predictor\n  reference_pos = map_lgl(\n    model_params, \n    ~ str_which(.x, \"b_femrebel*\") %>% \n      length() %>% \n      !.\n  ))\n\n# Apply names to the model fits list\nnames(bayes_model_fits) <- str_c(\"M\", 1:length(bayes_model_fits))\n\n# Extract the marginal likelihood object from each model\nmain_marglik <- map(bayes_model_fits, ~ .x$criteria$marglik)\n```\n\n### Female Rebels (Including SBs)\n\n```r\n# Posterior Probabilities for female rebel models\nfemrebels_marglik <- with(\n  params_pos_df,\n  main_marglik[c(\n    which(femrebels_pos == TRUE),\n    which(reference_pos == TRUE)\n  )]\n)\n\n# Matrix of Relative Posterior Probabilities\nfemrebels_post_probs <- do_call(\n  \"post_prob\", \n  args = unname(femrebels_marglik), \n  pkg = \"bridgesampling\"\n)\n\n# Apply model names to the matrix\ncolnames(femrebels_post_probs) <- names(femrebels_marglik) \n\n# Build a tibble for the comparisons\nfemrebels_vs_null_df <- tibble(\n  prob_femrebels = rowSums(femrebels_post_probs[, 1:128]),\n  prob_null = rowSums(femrebels_post_probs[, 129:255]),\n  Inclusion_BF = prob_femrebels/prob_null\n)\n```\n\n### Female Rebels (Excluding SBs)\n\n```r\n# Posterior Probabilities for female rebel models (excluding SBs)\nfemrebels_exs_marglik <- with(\n  params_pos_df,\n  main_marglik[c(\n    which(femrebelsexs_pos == TRUE),\n    which(reference_pos == TRUE)\n  )]\n)\n\n# Matrix of Relative Posterior Probabilities\nfemrebels_exs_post_probs <- do_call(\n  \"post_prob\", \n  args = unname(femrebels_exs_marglik), \n  pkg = \"bridgesampling\"\n)\n\n# Apply model names to the matrix\ncolnames(femrebels_exs_post_probs) <- names(femrebels_exs_marglik) \n\n# Build a tibble for the comparisons\nfemrebels_exs_vs_null_df <- tibble(\n  prob_femrebels_exs = rowSums(femrebels_exs_post_probs[, 1:128]),\n  prob_null = rowSums(femrebels_exs_post_probs[, 129:255]),\n  Inclusion_BF = prob_femrebels_exs/prob_null\n)\n```\n\n::: \n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n# Fancy ggplots because aesthetics\nbf_femrebels_vs_null <- ggplot(\n  data = femrebels_vs_null_df, \n  aes(x = Inclusion_BF)\n) +\n  # Slab stat for the raincloud plot\n  stat_slab(\n    aes(slab_alpha = stat(pdf)),\n    slab_color = \"#1BD0D5FF\",\n    fill_type = \"segments\",\n    fill = \"#2EF19CFF\",\n    show.legend = FALSE,\n    scale = 1\n  ) +\n  # Dot interval stat for the raincloud plot\n  stat_dotsinterval(\n    point_interval = \"median_qi\",\n    fill = \"#99FE42FF\",\n    shape = 23,\n    .width = c(0.68, 0.90),\n    point_size = 6,\n    stroke = 2,\n    slab_color = \"#1BD0D5FF\",\n    show.legend = FALSE,\n    side = \"bottom\",\n    layout = \"weave\",\n    dotsize = 0.75\n  ) +\n  # Add axis labels\n  labs(\n    title = \"Female Combatants vs. Null\",\n    x = parse(text = \"bold(Inclusion~BF[ij])\"),\n    y = \"\"\n  ) +\n  # Adjust the breaks on the x axis\n  scale_y_continuous(breaks = NULL) +\n  # Adjust the breaks on the y axis\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6))\n\n# Fancy ggplots because aesthetics\nbf_femrebels_exs_vs_null <- ggplot(\n  data = femrebels_exs_vs_null_df, \n  aes(x = Inclusion_BF)\n) +\n  # Slab stat for the raincloud plot\n  stat_slab(\n    aes(slab_alpha = stat(pdf)),\n    slab_color = \"#AB82FF\",\n    fill_type = \"segments\",\n    fill = \"#9400D3\",\n    show.legend = FALSE,\n    scale = 1\n  ) +\n  # Dot interval stat for the raincloud plot\n  stat_dotsinterval(\n    point_interval = \"median_qi\",\n    fill = \"#B23AEE\",\n    shape = 21,\n    .width = c(0.68, 0.90),\n    point_size = 6,\n    stroke = 2,\n    slab_color = \"#AB82FF\",\n    show.legend = FALSE,\n    side = \"bottom\",\n    layout = \"weave\",\n    dotsize = 0.75\n  ) +\n  # Add axis labels\n  labs(\n    title = \"Female Combatants (Excluding SBs) vs. Null\",\n    x = parse(text = \"bold(Inclusion~BF[ij])\"),\n    y = \"\"\n  ) +\n  # Adjust the breaks on the x axis\n  scale_y_continuous(breaks = NULL) +\n  # Adjust the breaks on the y axis\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6))\n\nbf_femrebels_vs_null|bf_femrebels_exs_vs_null\n```\n\n::: {.cell-output-display}\n![Inclusion Bayes Factors for Female Combatants](figs/fig-comparisons-bayes.png){#fig-comparisons-bayes fig-align='center' width=1728}\n:::\n:::\n\n\n::: a\nGiven the prior assumptions about the universe of effect sizes possible and prior probabilities for the hypothesis \nthat rebel groups in which women participate in combat during wartime are more likely to transition to political \nparties at wars end, the posterior inclusion probability for female combatants is approximately 0.841 when we \ninclude cases in which women only participated as suicide bombers and 0.936 if we code participation only in cases\nwhere women served in a combat capacity that was not explicitly limited to suicide bombing. Stated differently, \nas the inclusion Bayes Factors in figure @fig-comparisons-bayes illustrate, the observed data is between 5.29 \nand 14.64 times more likely to have arisen from those models that include a predictor for female combatants \ndepending on whether we consider cases in which women only participated in combat as suicide bombers or not.\nThis suggests a resonable degree of evidence in support the of the expectation that women's participation in \ncombat has a small to moderate effect on the probability of rebel to party transitions.\n:::\n\n## Model Averaged Marginal Effects\n\nTurning to the substantive effect estimate, I illustrate here three different approaches to estimating \nmodel averaged marginal effects. When fitting each of the models in the Stan program above, we applied\nBayesian g-formula to estimate the population average treatment effect in each of the models containing \nthe female combatants predictor and we can retreive those directly from the `brmsfit` objects. The\n`model_averaged_ame` function shown below implements the approach to averaging over a matrix of posterior\nprobability weights outlined in equation @eq-logit-bmame-reps along with the simpler approach from equation\n@eq-logit-binary-bmame.\n\n::: panel-tabset\n\n### Functions\n\n```r\n#' Estimating Model Averaged Marginal Effects and Contrasts for Bayesian Models\n#' \n#' This function facilitates the estimation of model averaged and stacked \n#' marginal effects for Bayesian models and is designed to account for uncertainty \n#' in the process of model specification and selection when estimating average \n#' marginal effects and probability contrasts.\n#'\n#' @aliases model_averaged_ame\n#' \n#' @import data.table\n#' @importFrom purrr map_dbl\n#'\n#' @param x A datatable of posterior draws for the average marginal effects\n#' or predictions such as that returned by the `posterior_contrasts` function.\n#'\n#' @param weights An \\eqn{n \\times m} matrix of posterior probability weights \n#' such as that returned by `bridgesampling::post_prob` or an m length vector\n#' of stacking or pseudo-BMA weights.\n#' \n#' @param summary A logical argument indicating whether to return the full \n#' draws for each row in `weights` or return the average for each model-draw \n#' pair. Defaults to `FALSE`\n#\n#' @param ... Additional arguments for future development, currently unused.\n#'\n#' @return A datatable containing a weighted average of the posterior draws \n#' for the model averaged marginal effects.\n#' \n#' @export model_averaged_ame\n#' \nmodel_averaged_ame <- function(x, weights, ndraws, summary = TRUE, ...) {\n  \n  # Nest the draws data frame\n  draws <- x[, list(x.yz = list(.SD)), by=model]\n  \n  # If weights are a matrix, calculate the draws by row\n  if (is.matrix(weights)) {\n    \n    # Construct a matrix of draw weights\n    weighted_draws <- apply(\n      weights, \n      MARGIN = 1, \n      FUN = function(x) {\n        brms:::round_largest_remainder(x * ndraws)\n      })\n    \n    # Initialize a list to store the draws in\n    out <- list()\n    \n    # Loop over each column in the matrix\n    for (i in 1:ncol(weighted_draws)) {\n      \n      # Randomly sample n rows for each model based on the weights\n      draw_ids <- lapply(weighted_draws[, i], function(x){\n        sample(sum(weighted_draws[,i]), size = x)\n      })\n      \n      # Randomly sample n draws proportional to the weights\n      out[[i]] <- lapply(seq_along(draws[, x.yz]), function(id) {\n        draws[, x.yz[[id]][draw_ids[[id]]]]\n      })\n      \n      # Bind the draws into a single data table per repetition\n      out[[i]] <- rbindlist(out[[i]], idcol = \"model\")\n    }\n    \n    # Combine everything into a single data table\n    out <- rbindlist(out, idcol = \"bridge_rep\")\n    \n    # Average over the bridge sampling repetitions by draw\n    if (isTRUE(summary)) {\n      out = out[\n        , keyby = .(.draw, model),\n        lapply(.SD, mean),\n        .SDcols = patterns(\"EY|AME\")\n      ]\n    }\n    \n  } else if (is.vector(weights)) {\n    # Construct a vector of draw weights\n    weighted_draws <- round_largest_remainder(weights * ndraws)\n    \n    # Randomly sample n rows for each model based on the weights\n    draw_ids <- lapply(weighted_draws, function(x){\n      sample(sum(weighted_draws), size = x)\n    })\n    \n    # Randomly sample n draws proportional to the weights\n    out <- lapply(seq_along(draws[, x.yz]), function(id) {\n      draws[, x.yz[[id]][draw_ids[[id]]]]\n    })\n    \n    # Combine everything into a single data table\n    out <- rbindlist(out, idcol = \"model\")\n  }\n  \n  # Return the model averaged draws\n  return(out)\n  \n}\n```\n\n### Female Rebels (Including SBs)\n\n```r\n# Set the seed here for reproducibility\nset.seed(2023)\n\n# Extract the AMEs from the models containing femrebels\nfemrebels_ame_boot <- map(\n  .x = with(\n    params_pos_df, \n    bayes_model_fits[which(femrebels_pos == TRUE)]\n    ),\n  ~ .x$predictions\n  ) %>%\n  # Append the list of data tables into one\n  rbindlist(., idcol = \"model\")\n\n# Posterior Probabilities for female rebel models\nfemrebels_bma_marglik <- with(\n  params_pos_df,\n  main_marglik[which(femrebels_pos == TRUE)]\n)\n\n# Matrix of Relative Posterior Probabilities for BMA\nfemrebels_bma_post_probs <- do_call(\n  \"post_prob\", \n  args = unname(femrebels_bma_marglik), \n  pkg = \"bridgesampling\"\n)\n\n# Apply model names to the matrix\ncolnames(femrebels_bma_post_probs) <- names(femrebels_bma_marglik) \n\n# Calculate the weighted draws for each model\nfemrebels_bmame <- model_averaged_ame(\n  femrebels_ame_boot, \n  weights = femrebels_bma_post_probs,\n  ndraws = 30e3,\n  summary = TRUE\n  )\n\n# Print the first few rows\nhead(femrebels_bmame)\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n     .draw model    EYX0    EYX1      AME\n  1:     1     2 0.74107 0.78571 0.044643\n  2:     1     3 0.59821 0.79464 0.196430\n  3:     1     4 0.58929 0.82143 0.232140\n  4:     1     5 0.57143 0.81250 0.241070\n  5:     1     9 0.54464 0.71429 0.169640\n  6:     1    11 0.58929 0.83036 0.241070\n```\n:::\n:::\n\n\n### Female Rebels (Excluding SBs)\n\n```r\n# Extract the AMEs from the models containing femrebels (excluding SBs)\nfemrebels_exs_ame_boot <- map(\n  .x = with(\n    params_pos_df, \n    bayes_model_fits[which(femrebelsexs_pos == TRUE)]\n    ),\n  ~ .x$predictions\n  ) %>%\n  # Append the list of data tables into one\n  rbindlist(., idcol = \"model\")\n\n# Posterior Probabilities for female rebel (excluding SBs) models\nfemrebels_exs_bma_marglik <- with(\n  params_pos_df,\n  main_marglik[which(femrebelsexs_pos == TRUE)]\n)\n\n# Matrix of Relative Posterior Probabilities for BMA\nfemrebels_exs_bma_post_probs <- do_call(\n  \"post_prob\", \n  args = unname(femrebels_exs_bma_marglik), \n  pkg = \"bridgesampling\"\n)\n\n# Apply model names to the matrix\ncolnames(femrebels_exs_bma_post_probs) <- names(femrebels_exs_bma_marglik) \n\n# Calculate the weighted draws for each model\nfemrebels_exs_bmame <- model_averaged_ame(\n  femrebels_exs_ame_boot, \n  weights = femrebels_exs_bma_post_probs,\n  ndraws = 30e3,\n  summary = TRUE\n  )\n\n# Print the first few rows\nhead(femrebels_exs_bmame)\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n     .draw model    EYX0    EYX1     AME\n  1:     1     1 0.48214 0.63393 0.15179\n  2:     1     3 0.39286 0.77679 0.38393\n  3:     1     4 0.39286 0.75000 0.35714\n  4:     1     6 0.56250 0.78571 0.22321\n  5:     1     7 0.49107 0.75000 0.25893\n  6:     1     8 0.48214 0.66071 0.17857\n```\n:::\n:::\n\n::: \n\n::: a\nWe can then plot the posterior distribution of the BMAMEs using `{ggdist}` as shown below which allows us \nto nicely depict the uncertainty in the estimates. As @fig-model-averaged-ame-boot illustrates, the model\naveraged marginal effect estimates for the expected difference in the probability of former combatant\ngroup transitioning to a post-conflict political party is positive for both measures. When considering \nonly cases where women served in a combat capacity that was not explicitly limited to suicide bombing, \nwe see a 98.38% probability of the effect being positive and in the expected direction with a posterior \nmedian of 0.2232 and 89% credible interval spanning the range 0.0625 and 0.3750. \n\nThe results are largely identical for the other measure that includes those cases in which female \ncombatants only served as suicide bombers as instances of participation, with a 96.32% probability \nof the effect being positive and a posterior median of of 0.1875 and an 89% probability the parameter \nvalue falls between 0.0268 and 0.3482. Overall, the general conclusion here is rebel groups where \nwomen particpate in combat roles during wartime are more likely to form parties and compete in \npost-conflict elections.\n::: \n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n# Append these into a single dataframe for plotting\nbmame_boot_ests <- rbindlist(\n  list(\n    \"Female Combtants (Excluding SB Roles)\" = femrebels_exs_bmame, \n    \"Female Combtants (Including SB Roles)\" = femrebels_bmame\n    ), \n  idcol = \"plot_row\"\n  )\n\n# Initialize the ggplot2 object for the model averaged AMEs\nggplot(bmame_boot_ests, aes(y = AME, x = plot_row)) +\n  # Violin plot looking things\n  stat_eye(\n    aes(\n      slab_alpha = after_stat(pdf), \n      slab_fill = stat(y > 0), \n      point_fill = plot_row, \n      shape = plot_row\n    ),\n    slab_size = 2,\n    fill_type = \"gradient\",\n    point_interval = \"median_qi\",\n    .width = c(0.68, 0.89),\n    point_size = 6,\n    stroke = 2,\n    show.legend = FALSE, \n    scale = 0.3\n  ) +\n  # Adjust the y axis scale\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-0.4, 0.65)\n    ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#23C3E4FF\", \"#C82803FF\"), \n    aesthetics = \"slab_fill\"\n  ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#3AA2FCFF\", \"#FD8A26FF\"), \n    aesthetics = \"point_fill\"\n  ) +\n  # Adjust shape scale\n  scale_shape_manual(values = c(22, 21)) +\n  # Add labels to the plot\n  labs(\n    y = parse(text = \"bold('Model Averaged AME '*Delta)\"),\n    x = \"\"\n    )\n```\n\n::: {.cell-output-display}\n![Model Averaged Marginal Effects and Expectations for the Effect of Female Combatants on the Probability of Post-Conflict Party Transitions](figs/fig-model-averaged-ame-boot.svg){#fig-model-averaged-ame-boot fig-align='center' width=1344}\n:::\n:::\n\n\n::: a\nAlthough this is the ideal approach since it accounts for the additional source of computational uncertainty\nintroduced by approximating the marginal likelihood, it can be expensive in terms of both computational time \nand memory requirements in addition to requiring a minimal knowledge of Stan. We can sidestep Stan and focus\nexclusively on `{brms}` by defining a simple R function to calculate contrasts and model averaged \nmarginal effects as shown below. As figure @fig-model-averaged-ame-alt illustrates, the estimates here are \nvirtually identical and do not differ substantively from those in figure @fig-model-averaged-ame-boot. \nThis estimation process is reasonably straightforward and can be used for any model supported \nby `{brms}`.\n:::\n\n::: panel-tabset\n\n### Functions\n\n```r\n#' Posterior Contrasts for Bayesian Models fit with {brms}\n#'\n#' @param models A named list of model objects of class `brmsfit` for which to \n#' generate predictions.\n#' \n#' @param X A string indicating the name of the treatment in the models.\n#' \n#' @param data Data used to fit the model objects.\n#' \n#' @param contrasts Values of `X` at which to generate predictions. Defaults to\n#' `c(0, 1)` but could also be set to factor levels.\n#' \n#' @param cores Number of cores to use for parallel computation. If cores > 1,\n#' future::multisession is used to speed computation via `{furrr}`. Requires\n#' the `{furrr}` package.\n#' \n#' @param ... Additional arguments passed down to `brms::posterior_predict`\n#'\n#' @return A list of posterior contrasts for each model in `models`\n#' \n#' @export posterior_contrasts\n#'\nposterior_contrasts <- function(models, \n                                X, \n                                data,\n                                contrasts = c(0, 1),\n                                cores = 1,\n                                ...) {\n  \n  # Check that all elements of models are of class brmsfit\n  for (i in seq_along(models)) {\n    brms_check <- brms::is.brmsfit(models[[i]])\n    stopifnot(\n      \"All elements in models must be of class brmsfit\" = brms_check, \n      brms_check\n      )\n  }\n  \n  # Y_i(X = 0, Z)\n  lo <- data |> mutate(\"{X}\" := contrasts[1])\n  \n  # Y_i(X = 1, Z)\n  hi <- data |> mutate(\"{X}\" := contrasts[2])\n  \n  # Check if cores > 1\n  if (cores > 1) {\n    \n    # Requires {furrr}\n    require(furrr)\n    \n    # Fit models in parallel via future\n    plan(tweak(multisession, workers = cores))\n    \n    # Posterior predictions for each data set\n    predictions <- furrr::future_map(\n      .x = models,\n      ~ .get_posterior_predictions(hi, lo, model = .x, ...),\n      .options = furrr::furrr_options(\n        scheduling = 1,\n        seed = TRUE,\n        prefix = \"prefix\"\n      ),\n      .progress = TRUE\n    )\n    \n    # Close the future session\n    plan(sequential)\n  }\n  \n  if (cores == 1) {\n    # Posterior predictions for each data set\n    predictions <- purrr::map(\n      .x = models,\n      ~ .get_posterior_predictions(hi, lo, model = .x, ...)\n    )\n  }\n  \n  # Return the list of predictions\n  return(predictions)\n}\n\n# Helper function to get posterior prediction contrasts\n.get_posterior_predictions <- function(hi, lo, model, ...) {\n  \n  # Predictions for Y_i(X = 1, Z)\n  EYX1 <- brms::posterior_predict(model, newdata = hi, ...)\n  \n  # Predictions for Y_i(X = 0, Z)\n  EYX0 <- brms::posterior_predict(model, newdata = lo, ...)\n  \n  # Average over the observations for each draw in the prediction matrix\n  out <- data.table(\n    EYX1 = rowMeans(EYX1),\n    EYX0 = rowMeans(EYX0),\n    AME = rowMeans(EYX1 - EYX0),\n    .draw = 1:ndraws(model)\n  )\n  \n  # Return just the average contrast\n  return(out)\n}\n```\n\n### Female Rebels (Including SBs)\n\n```r\n# Models containing femrebels\nfemrebels_mods <- with(\n  params_pos_df,\n  bayes_model_fits[which(femrebels_pos == TRUE)]\n)\n\n# Generate posterior expectations for the AMEs\nfemrebels_ame_draws <- posterior_contrasts(\n  models = femrebels_mods, \n  X = \"femrebels\", \n  data = femrebels_mods[[1]]$data,\n  contrasts = c(0, 1),\n  cores = 4\n)\n\n# Calculate the weighted draws for each model\nfemrebels_bmame_draws <- model_averaged_ame(\n  femrebels_ame_draws, \n  weights = femrebels_bma_post_probs,\n  ndraws = 30e3,\n  summary = TRUE\n  )\n\n# Print the first few rows\nhead(femrebels_bmame_draws)\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n     .draw model      EYX1      EYX0       AME\n  1:     1     2 0.8214286 0.6160714 0.2053571\n  2:     1     3 0.7678571 0.5000000 0.2678571\n  3:     1     4 0.7857143 0.4375000 0.3482143\n  4:     1     5 0.7857143 0.5803571 0.2053571\n  5:     1     7 0.6428571 0.5803571 0.0625000\n  6:     1     9 0.7946429 0.6339286 0.1607143\n```\n:::\n:::\n\n\n### Female Rebels (Excluding SBs)\n\n```r\n# Models containing femrebels\nfemrebels_exs_mods <- with(\n  params_pos_df,\n  bayes_model_fits[which(femrebelsexs_pos == TRUE)]\n)\n\n# Generate posterior expectations for the AMEs\nfemrebels_exs_ame_draws <- posterior_contrasts(\n  models = femrebels_exs_mods, \n  X = \"femrebels_exs\", \n  data = femrebel_exs_mods$M112$data,\n  contrasts = c(0, 1),\n  cores = 4\n)\n\n# Calculate the weighted draws for each model\nfemrebels_exs_bmame_draws <- model_averaged_ame(\n  femrebels_exs_ame_draws, \n  weights = femrebels_exs_bma_post_probs,\n  ndraws = 30e3,\n  summary = TRUE\n  )\n\n# Print the first few rows\nhead(femrebels_exs_bmame_draws)\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n     .draw model      EYX1      EYX0       AME\n  1:     1     1 0.6875000 0.5446429 0.1428571\n  2:     1     2 0.7142857 0.4642857 0.2500000\n  3:     1     3 0.8035714 0.4464286 0.3571429\n  4:     1     4 0.7321429 0.4464286 0.2857143\n  5:     1     5 0.8839286 0.6250000 0.2589286\n  6:     1     7 0.6785714 0.4464286 0.2321429\n```\n:::\n:::\n\n::: \n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\nfemrebels_bmame_plot_alt <- femrebels_bmame_draws |>\n  # Initialize the ggplot2 object for the model averaged AME\n  ggplot(aes(x = AME)) +\n  # Add a slab interval to plot the weighted posterior\n  stat_halfeye(\n    aes(\n      slab_alpha = after_stat(pdf),\n      slab_fill = stat(x > 0)\n      ),\n    slab_size = 2,\n    slab_colour = \"#1BD0D5FF\",\n    shape = 22,\n    fill_type = \"gradient\",\n    point_interval = \"median_qi\",\n    .width = c(0.68, 0.89),\n    point_size = 6,\n    stroke = 2,\n    point_fill = \"#99FE42FF\",\n    show.legend = FALSE\n  ) +\n  # Add labels to the plot\n  labs(\n    x = parse(text = \"bold('Model Averaged AME '*Delta)\"),\n    y = \"Posterior Density\",\n    title = \"Female Combtants (Including Suicide Bomber Roles)\"\n  ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#208820\", \"#0060B0\"), \n    aesthetics = \"slab_fill\"\n  ) +\n  # Adjust the x axis scale\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-0.4, 0.68)\n    ) +\n  # Adjust the y axis scale\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6))\n\nfemrebels_exs_bmame_plot_alt <- femrebels_exs_bmame_draws |>\n  # Initialize the ggplot2 object for the model averaged AME\n  ggplot(aes(x = AME)) +\n  # Add a slab interval to plot the weighted posterior\n  stat_halfeye(\n    aes(\n      slab_alpha = after_stat(pdf), \n      slab_fill = stat(x > 0)\n      ),\n    slab_size = 2,\n    slab_colour = \"#1BD0D5FF\",\n    shape = 22,\n    fill_type = \"gradient\",\n    point_interval = \"median_qi\",\n    .width = c(0.68, 0.89),\n    point_size = 6,\n    stroke = 2,\n    point_fill = \"#99FE42FF\",\n    show.legend = FALSE\n  ) +\n  # Add labels to the plot\n  labs(\n    x = parse(text = \"bold('Model Averaged AME '*Delta)\"),\n    y = \"Posterior Density\",\n    title = \"Female Combtants (Excluding Suicide Bomber Roles)\"\n  ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#208820\", \"#0060B0\"), \n    aesthetics = \"slab_fill\"\n  ) +\n  # Adjust the x axis scale\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-0.4, 0.68)\n    ) +\n  # Adjust the y axis scale\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6))\n\n(femrebels_bmame_plot_alt / femrebels_exs_bmame_plot_alt)\n```\n\n::: {.cell-output-display}\n![Model Averaged Marginal Effects and Expectations for the Effect of Female Combatants on the Probability of Post-Conflict Party Transitions, Direct Estimation](figs/fig-model-averaged-ame-alt.svg){#fig-model-averaged-ame-alt fig-align='center' width=1536}\n:::\n:::\n\n\n## Illustration in {marginaleffects}\n\nFinally, in the interest of encouraging applied researchers to take uncertainty seriously and integrate sensitivity \nanalysis into their work directly rather than treating it as something to undertake only when forced, I turn \nnow to a straightforward illustration of BMAMEs using the `{marginaleffects}` package which provides the \nnecessary functionality to handle most of the above steps automatically thanks to the feature-rich support for \nvarious approaches to averaging across posterior distributions provided by `{brms}`.\n\n::: a\nTo obtain the BMAME for a given parameter while accounting for the uncertainty in the model specifications, \nversion 0.5.0 and higher of `{marginaleffects}` allows users to specify the argument `type = \"average\"` in \ntheir call to either `marginaleffects::marginaleffects` or `marginaleffects::comparisons` for objects of \nclass `brmsfit` along with any additional arguments to be passed down to `pp_average` such as the type of \nweights to estimate, or alternatively a numeric vector of pre-estimated weights which is usually the more \ncomputationally efficient option and the approach I take in the example below.\n\nAlthough up to this point I have been averaging over all possible combinations of models containing the\nparameter of interest, we will take a step back here and focus on the full model specifications, averaging \nover two models that differ only in which of the female combatants measures they contain. I begin by \nspecifying and estimating each of the models using `{brms}`.\n::: \n\n::: panel-tabset\n\n### Specification\n\n```r\n# Prepare the data, we need the two variables to be named the same\nmodel_data <- list(\n  \"femrebels\" = model_df %>% \n    mutate(femrebel = femrebels),\n  \"femrebels_exs\" = model_df %>% \n    mutate(femrebel = femrebels_exs)\n)\n\n# Model formula for the female rebels models\nfemrebel_bf <- bf(\n  elect_party ~ femrebels + left_ideol + nationalist + un_intervention + \n    party_affil + ethnic_group + un_intervention + africa + qsi_pol + \n    qsi_econ + qsi_social\n  )\n\n# Specify priors for the model parameters\nfemrebel_logit_priors <- prior(normal(0, 0.75), class = b) +\n  prior(normal(0, 1), class = Intercept)\n\n# Define the custom stanvars for the bootstrapped g-formula for comparison purposes\ngformula_vars <- stanvar(\n  x = 1,\n  name = \"treat_pos\",\n  block = \"data\",\n  scode = \"int<lower = 1, upper = K> treat_pos; // Treatment Position\"\n) + \n  # Code for the transformed data block\n  stanvar(\n    scode = \"\n  // Bootstrap Probabilities\n  vector[N] boot_probs = rep_vector(1.0/N, N);\n  \n  // Matrices for the bootstrapped predictions\n  matrix[N, Kc] Xa; \n  matrix[N, Kc] Xb;\n    \n  // Potential Outcome Y(X = 1, Z)\n  Xa = X[, 2:K];\n  Xa[, treat_pos] = ones_vector(N);\n    \n  // Potential Outcome Y(X = 0, Z)\n  Xb = X[, 2:K];\n  Xb[, treat_pos] = zeros_vector(N);\n    \",\n  position = \"end\",\n  block = \"tdata\"\n  ) +\n  # Code for the bootstrapped g-formula in generated quantities\n  stanvar(\n    scode = \"\n  // Row index to be sampled for bootstrap\n  int row_i;\n    \n  // Calculate Average Marginal Effect in the Bootstrapped sample\n  real AME = 0;\n  array[N] real Y_X1; // Potential Outcome Y(X = 1, Z)\n  array[N] real Y_X0; // Potential Outcome Y(X = 0, Z)\n    \n  for (n in 1:N) {\n    // Sample the Baseline Covariates\n    row_i = categorical_rng(boot_probs);\n      \n    // Sample Y(x) where x = 1 and x = 0\n    Y_X1[n] = bernoulli_logit_rng(b_Intercept + Xa[row_i] * b);\n    Y_X0[n] = bernoulli_logit_rng(b_Intercept + Xb[row_i] * b);\n      \n    // Add Contribution of the ith Observation to the Bootstrapped AME\n    AME = AME + (Y_X1[n] - Y_X0[n])/N;\n  }\n    \n  // Take the mean of the posterior expectations\n  real EYX1 = mean(Y_X1); // E[Y | X = 1, Z]\n  real EYX0 = mean(Y_X0); // E[Y | X = 0, Z]\n    \",\n  position = \"end\",\n  block = \"genquant\"\n  )\n```\n\n### Estimation\n\n```r\n# Fit each of the models (6 chains, 8k iterations), takes about 3 seconds\nfemrebels_logit_fits <- map2(\n  .x = model_data,\n  .y = str_c(\"Party_Transition_Logit_\", names(model_data),  \".rds\"),\n  .f = ~ brm(\n    formula = femrebel_bf,\n    family = brmsfamily(\"bernoulli\", link = \"logit\"),\n    prior = femrebel_logit_priors,\n    data = .x,\n    cores = 12, # Max number of cores to use for parallel chains\n    chains = 6, # Number of chains, should be at least 4\n    iter = 8000, # Total iterations = Warm-Up + Sampling\n    warmup = 3000, # Warm-Up Iterations\n    stanvars = gformula_vars, # Custom Stan code to include in the model\n    save_pars = save_pars(all = TRUE),\n    seed = 2023,\n    backend = \"cmdstanr\",\n    sample_prior = \"yes\",\n    file = .y\n  )\n)\n\n# Add PSIS LOO-CV and Marginal Likelihood\nfemrebels_logit_fits <- map(\n  .x = femrebels_logit_fits,\n  .f = ~ add_criterion(\n    .x,\n    criterion = c(\"loo\", \"marglik\"),\n    max_iter = 5e3,\n    repetitions = 25,\n    cores = 8,\n    silent = TRUE\n  ))\n```\n::: \n\n\n::: a\nWe can then use `bridgesampling::post_prob` to obtain the relative probability of each model based on\nthe marginal likelihood estimates and `brms::loo_model_weights` to obtain cross-validation based \nstacking and pseudo-BMA weights. We can then use `comparisons` to estimate the model averaged probability\ncontrasts.\n:::\n\n::: panel-tabset\n\n### Model Weights\n\n```r\n# Get the posterior probability weights\npostprob_weights <- bridgesampling::post_prob(\n  femrebels_logit_fits[[1]]$criteria$marglik, # Full Model, Including Suicide Bombers\n  femrebels_logit_fits[[2]]$criteria$marglik, # Full Model, Excluding Suicide Bombers\n  prior_prob = c(0.5, 0.5), # This is the default, I'm just being explicit\n  model_names = c(\"femrebels\", \"femrebels_exs\")\n)\n\n# Calculate Pseudo-BMA+ Weights\npbma_weights <- loo_model_weights(\n  femrebels_logit_fits[[1]],\n  femrebels_logit_fits[[2]],\n  method = \"pseudobma\",\n  model_names = names(femrebels_logit_fits),\n  BB = TRUE, # Pseudo-BMA+ with Bayesian Bootstrap\n  BB_n = 10000, # Use 10k replications for Bayesian Bootstrap\n  cores = 8L\n)\n\n# Calculate Stacking Weights\nstacking_weights <- loo_model_weights(\n  femrebels_logit_fits[[1]],\n  femrebels_logit_fits[[2]],\n  method = \"stacking\",\n  model_names = names(femrebels_logit_fits),\n  optim_control = list(reltol = 1e-10),\n  cores = 8L\n)\n\n# Store all the weights in a named list\nweights_ls <- list(\n  \"BMA\" = apply(postprob_weights, 2, mean),\n  \"Stacking\" = stacking_weights,\n  \"Pseudo-BMA+\" = pbma_weights\n)\n\n# Print a comparison of the weights\nprint(weights_ls)\n```\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n  $BMA\n      femrebels femrebels_exs \n      0.2898716     0.7101284 \n  \n  $Stacking\n  Method: stacking\n  ------\n                weight\n  femrebels     0.000 \n  femrebels_exs 1.000 \n  \n  $`Pseudo-BMA+`\n  Method: pseudo-BMA+ with Bayesian bootstrap\n  ------\n                weight\n  femrebels     0.255 \n  femrebels_exs 0.745\n```\n:::\n:::\n\n\n### Comparisons\n\n```r\n# Calculate model averaged contrasts\ncomparisons_femrebels <- map(\n  .x = weights_ls,\n  ~ comparisons(\n    femrebels_logit_fits[[1]],\n    m1 = femrebels_logit_fits[[2]], # Subsequent models must be named arguments\n    variables = \"femrebel\",\n    type = \"average\",\n    transform_pre = \"differenceavg\",\n    weights = .x,\n    method = \"posterior_predict\" # Accounts for observation-level uncertainty\n  )\n)\n\n# Extract the posterior draws\ncomparisons_femrebels_draws <- map(\n  .x = comparisons_femrebels,\n  ~ posteriordraws(.x, shape = \"long\")\n)\n\n# Bind the draws frames into a data table\ncomparisons_femrebels_draws <- rbindlist(\n  comparisons_femrebels_draws,\n  idcol = \"type\"\n  )\n```\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: \n\n::: a\nFigure @fig-model-averaged-ame-margins shows a side by side comparison of the weighted average of the \ncontrasts based on different weighting approaches. The results here are largely the same as those above,\nbut we have combined the two measures into a single model-wise mixture distribution and can incorporate \nwhat is effectively measurement uncertainty here directly into the quantity of interest. The `{marginaleffects}` \nimplementation is somewhat limted by its dependence on `brms::pp_average` and it is on my list of things to \ndo for the coming year to work out a more efficient approach that allows for the efficiency and flexibility \nof those illustrated in the preceding section.\n:::\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n# Initialize the ggplot2 object for the model averaged AMEs\ncomparisons_femrebels_draws %>% \nggplot(aes(y = draw, x = weights)) +\n  # Violin plot looking things\n  stat_eye(\n    aes(\n      slab_alpha = after_stat(pdf), \n      slab_fill = weights, \n      point_fill = weights, \n      shape = weights\n    ),\n    slab_size = 2,\n    fill_type = \"gradient\",\n    point_interval = \"median_qi\",\n    .width = c(0.68, 0.89),\n    point_size = 6,\n    stroke = 2,\n    show.legend = FALSE, \n    scale = 0.8\n  ) +\n  # Adjust the y axis scale\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-0.19, 0.54)\n  ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#23C3E4FF\", \"#C82803FF\", \"#2EF19CFF\"), \n    aesthetics = \"slab_fill\"\n  ) +\n  # Adjust fill scale\n  scale_fill_manual(\n    values = c(\"#3AA2FCFF\", \"#FD8A26FF\", \"#99FE42FF\"), \n    aesthetics = \"point_fill\"\n  ) +\n  # Adjust shape scale\n  scale_shape_manual(values = c(22, 21, 24)) +\n  # Add labels to the plot\n  labs(\n    y = parse(text = \"bold('Model Averaged AME '*Delta)\"),\n    x = \"\"\n  )\n```\n\n::: {.cell-output-display}\n![Model Averaged Marginal Effects for the Effect of Female Combatants on the Probability of Post-Conflict Party Transitions, {marginaleffects} Implementation](figs/fig-model-averaged-ame-margins.svg){#fig-model-averaged-ame-margins fig-align='center' width=1344}\n:::\n:::\n\n\n## Conclusion\n\nThis concludes what was hopefully a detailed illustration of model averaged marginal \neffects with `{brms}`, `{marginaleffects}`, and Stan. The general takeaway here is \nthat selecting a single model is usually the wrong approach and there is seldom any \nneed to do so. While no method of estimation is perfect, BMA and its variants \nprovide a principled, relatively robust way to propagate uncertainty in quantities \ntypically of interest to researchers in the social sciences and can help us avoid \nunnecessary dichotomies that are, in most cases, false. By applying probability \ntheory to average across posterior distributions when estimating average marginal \neffects, we can obtain interpretable model averaged estimates for both linear and \nnon-linear regression models.\n\n::: a\nAs far as subsequent developments are concerned, this is effectively the cornerstone\nof my dissertation work so I am hoping to have at least one subsequent post \nillustrating and implementing BMAMEs for multilevel models that employ non-linear \nlink functions in the comining weeks. In the meantime, however, remember that nothing\never good comes from mixing odds and ratios or exposing your relatives to unnecessary \nrisks and holding the random effects in a hierarchical model at their mean still gives\nyou a conditional estimate that results in a weak connection between theory and data.\nFinally, if you catch any mistakes--and I can almost guarentee there are many in a \npost of this length--either leave a comment or let me know some other way (i.e., twitter).\n:::\n\n# References",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}