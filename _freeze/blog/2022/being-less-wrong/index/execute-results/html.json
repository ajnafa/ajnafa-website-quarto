{
  "hash": "d7175eab26a3a6b04ae5518d91ea078c",
  "result": {
    "markdown": "---\ntitle: The Art of Being Less Wrong\nsubtitle: An Introduction to Bayesian Model Averaged Marginal Effects\npagetitle: The Art of Being Less Wrong\ndate: 2022-05-24\ndescription: \"This post provides an introduction to Bayesian Model Averaging and Model Averaged Marginal Effects with Stan, `{brms}`, and `{marginaleffects}`\"\ncategories: [R, Bayesian Statistics, Causal Inference, Marginal Effects, Stan, Logit, Model Averaging]\nlicense: All content in this post is made available for public use under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License.\nimage: figs/preview-fig.png\ntoc-location: left\npage-layout: full\n---\n\n\n\n\n::: callout-important\n### Work in Progress\n\nThis is a draft post and under is currently under development. I have not proofread any of the contents herein\n\n:::\n\n# Introduction\n\nGeorge E. P. Box famously described the problem of model development in statistics and related fields by noting, \n*all models are wrong, but some models are useful* [-@Box1976]. That is, all models are necessarily simplifications \nofa much more complex reality and statistics is by no means an algorithmic truth generating process because no \nmodel can ever be true in the pure sense of the word. Yet, this at times underappreciated reality does not render \nall models useless but rather implies the goal of those who practice statistics in its various forms is to \ndevelop and identify models that are useful in a never ending quest to be less wrong. In this post I introduce an \napproach to this task that has been the subject of my own dissertation work on the use of Bayesian Model Averaging \n(BMA) to account for uncertainty in model specification when estimating average marginal effects in non-linear \nregression models. \n\n::: a \nI introduce what I call a Bayesian Model Averaged Marginal Effect (BMAME) in the context of logistic regression \nmodels in political science and illustrate how to obtain BMAMEs in a straightforward manner using the \n`{marginaleffects}` R package [@ArelBundock2022a], which thanks to Vincent Arel-Bundock's assistance supports model averaged or\nstacked average marginal effects for any model fit with the Stan interface `{brms}` \n[@Burkner2017; @Burkner2018]. I also provide a more general demonstration in Stan that makes\nthe method easily applicable in Python and high dimensional settings. Although I focus here on models that include \nonly population-level effects, recent feature additions to `{marginaleffects}` now make it possible to obtain\nBMAMEs from more complex hierarchical models and I will cover such applications in a subsequent post.\n::: \n\n# Bayesian Model Averaging in Political Science\n\nAlthough Bayesian Model Averaging was introduced to political science more than two decades ago \nby @Bartels1997, cases of its use in political science remain rare and are largely confined to the topic \nof political methodology [@Juhl2019; @Cranmer2015; @Montgomery2010].[^1] This presents something of a \nproblem given the numerous studies demonstrating the dangers of tossing a large number of likely correlated \npredictors into a regression model [@Achen2005; @Clarke2005; @Clarke2009; @Montgomery2018]; the reality that \nif we wish to adjudicate between two or more competing theories, comparing coefficients in a single model is \ngenerally insufficient to accomplish such a task [@Imai2011; @Clarke2007; @Hollenbach2020]; and the difficulty \nof assessing what we truly known about political phenomenon that results from an obsession with \"statistical \nsignificance\" and precise answers to poorly defined questions [@Ward2010; @Schrodt2014; @Lundberg2021]. \n\n[^1]: For a recent application of BMA in the context of instrument selection see @Rozenas2019.\n\n::: a \nIn contrast to the advice of @Montgomery2010 who suggest \"BMA is best used as a subsequent robustness \ncheck to show that our inferences are not overly sensitive to plausible variations in model specification\" \n(266), I argue here that model averaging can and should be used as far more than a robustness check if for\nno other reason than because a model-wise mixture distribution for our paramter of intertest is almost \ncertainly more informative than a point estimate from a single model specification. Indeed, BMA provides\none of the few natural and intuitive ways of resolving some of the issues outlined in the preceding \nparagraph, particularly as it pertains to the evaluation of non-nested theoretical models, while also\nexhibiting a lower false positive rate than alternative approaches [@Pluemper2018]. This section provides \na brief explanation of BMA and its relation to what I term model averaged marginal effects before \nproceeding to a discussion of how the procedure is implemented.\n::: \n\n## Bayesian Model Comparison and Model Averaging\n\nConsider a simple case in which we have a set of $\\mathcal{M}$ models, each of which characterizes a \npossible representation of some unknown data generation process. Although it remains common practice \nin applied political science to select a single model based on a largely arbitrary fit statistic \n(i.e., AIC, $\\chi^{2}$, or whatever people are getting a p-value from this week), such an approach is \nproblematic since it ignores the uncertainty inherent in the process of model specification. Rather \nthan selecting a single model and consequently placing an implicit zero prior on \n**every other possible model specification you could have considered but did not**, Bayesian Model \nAveraging and its analogues provide a means of averaging across several possible, potentially \nnon-nested, model specifications and accounting for the uncertainty associated with doing so.\n\n::: a \nImagine we wish to adjudicate between two competing theoretical models in the set of candidate models \n$\\mathcal{M}$. The posterior odds of model $i$ relative to an alternative $k$ can be expressed as\n:::\n\n$$\n\\underbrace{\\frac{\\Pr(\\mathcal{M}_{i} \\, | \\, y)}{\\Pr(\\mathcal{M}_{k} \\, | \\, y)}}_{\\mathrm{Posterior~Odds}} = \\underbrace{\\frac{p(y \\, | \\, \\mathcal{M}_{i})}{p(y \\, | \\, \\mathcal{M}_{k})}}_{\\mathrm{Bayes~Factor}} \\times \\underbrace{\\frac{\\pi(\\mathcal{M}_{i})}{\\pi(\\mathcal{M}_{k})}}_{\\mathrm{Model~Prior}}\n$$ {#eq-posterior-odds} \n\nwhere $\\pi(\\mathcal{M})$ is the prior probability of model $i$ over model $k$ and $p(y \\,|\\, \\mathcal{M})$ \nis the marginal likelihood of the observed data under each of the candidate models such that\n\n$$\n\\underbrace{p(y \\,|\\, \\mathcal{M})}_{\\mathrm{Marginal~Likelihood}} = \\int\\underbrace{p(y \\,|\\, \\theta,\\, \\mathcal{M})}_{\\mathrm{Likelihood}} \\, \\underbrace{\\pi(\\theta \\,|\\, \\mathcal{M})}_{\\mathrm{Prior}}d\\theta\n$$ {#eq-marginal-likelihood}\n\n::: a \nGiven our prior assumptions about how likely the observed data are to have arisen from each model in the set \n$\\mathcal{M}$, the result of equation @eq-posterior-odds is the posterior odds of model $i$ compared to model \n$k$ and thus captures the relative probability that model $\\mathcal{M_{i}}$ represents the true data generation \nprocess compared to the plausible alternative $\\mathcal{M_{k}}$. We can easily extend this example to the setting \nin which $\\mathcal{M_{k}}$ is a set of plausible competing models of size $k > 2$, in which case the posterior \nmodel probability of the $i^{th}$ model relative to each of the alternatives $k$ is\n:::\n\n$$\n\\Pr(\\mathcal{M}_{i} \\,|\\, y) = \\frac{p(y \\, | \\, \\mathcal{M}_{i}) \\, \\cdot \\, \\pi(\\mathcal{M}_{i})}{\\displaystyle\\sum^{\\mathcal{m}}_{k=1} p(y \\, | \\, \\mathcal{M}_{k}) \\, \\cdot \\, \\pi(\\mathcal{M}_{k})}\n$$ {#eq-posterior-probability} \n\nwhich provides the posterior model probability of model $i$ relative to each of the alternative candidate \nspecifications $k$. Applying equation @eq-posterior-probability to each model in the set under consideration \nyields a vector of relative posterior model probabilities of length $m$ which may be used as either posterior \nprobability weights as in the case of Bayesian Model Averaging or for the estimation of posterior inclusion \nprobabilities.\n\n::: a \nIn the context of model averaging, we can take draws from the posterior predictive distribution of each model \ncontaining a given parameter of interest proportional to its posterior probability weight obtained from equation \n@eq-posterior-probability. This yields a model-wise mixture representing a weighted average of the posterior \npredictive distribution for some parameter such as a regression coefficient for a linear model or as I outline \nbelow, an average marginal effect or probability contrast for models that employ a non-linear link function. \n::: \n\n## Bayesian Model Averaged Marginal Effects\n\nAverage marginal effects are ubquitous in the social sciences though, as \n[Andrew Heiss nicely illustrates](https://www.andrewheiss.com/blog/2022/05/20/marginalia/), the term average \nmarginal effect is often used ambiguously and in the context of more complex models, the terms marginal and\nconditional [tend to be a source of additional confusion](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/). \nIn the case of a simple population-level linear model with a Gaussian likelihood and identity link function, \nthe model averaged marginal effect is equivalent to the posterior distribution of the model averaged \ncoefficient $\\beta$ which can be expressed as\n\n$$\n\\mathbb{E}(\\beta \\, | \\, y) = \\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} \\, | \\, y) \\cdot \\mathbb{E}(\\beta\\, |\\, \\mathcal{M}_{i}, \\, y) \\quad \\forall \\quad i \\in \\{1, 2,\\dots, m\\}\n$$ {#eq-bma-population} \n\nwhere $\\Pr(\\mathcal{M}_{i} ~ | ~ y)$ represent the posterior model probability and \n$\\mathbb{E}(\\beta ~|~ \\mathcal{M}_{i}, ~ y)$ is the expected value of some parameter $\\beta$ \nconditional on the observed data and some model specification for each model $i$ in the set of \nmodels under consideration $m$ [@Montgomery2010].\n\n::: a\nFor probability models such as logit, and other commonly used formulations for modeling\ndiscrete responses, this simple closed form solution does not exist nor is there any meaningful\nway to interpret the coefficients of these models as efffect size estimates--a reality that unfortunately \nremains quite widely misunderstood [@Mood2009; @Daniel2020]. As a general principle, any model more\ncomplex than a simple linear regression can only be meaningfully interpreted in terms of the predictions\nit generates [@Long2018], which in the case of logistic regression simply means applying the inverse \nlogistic function to obtain predicted probabilities which we can then use to estimate contrasts or average \nmarginal effects [@Norton2019]. Letting $\\mathrm{Y}$ represent a binary outcome, $\\mathrm{X}$ a continuous \nexposure or treatment of interest, and $\\mathrm{Z}$ a matrix of measured confounders, we can express the \naverage marginal effect as\n:::\n\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\frac{\\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij} + h, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij}, \\mathrm{Z}_{ij}]d\\mathrm{Z}}{h} \\\\\n$$ {#eq-logit-continuous-ame} \n\nwhich provided a sufficiently small value of $h$, yields a reasonable approximation of the partial derivative \nand provides a posterior distribution for the average effect of an instantaneous change in $\\mathrm{X}$ on the\nprobability scale similar to what one obtains in linear regression.[^2]\n\n[^2]: In the case of an interval exposure, it may make more sense to estimate a marginal effect at each unique or representative value of $\\mathrm{X}$ to better capture non-linearities.\n\n::: a\nSimilarly, for a binary treatment of interest we apply the Bayesian g-formula to obtain the average marginal \neffect of $\\mathrm{X}$ on the probability of $\\mathrm{Y}$ as shown in equation @eq-logit-binary-ame.\n:::\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n$$ {#eq-logit-binary-ame}\n\nEquations @eq-logit-continuous-ame and @eq-logit-binary-ame make clear that we are averaging over the confounder \ndistribution $\\mathrm{Z}$ rather than holding it constant at some fixed value [@Oganisian2020; @Keil2017]. \nThis point is an important one because in binary outcome models such as logit and probit, holding $\\mathrm{Z}$ \nconstant at some fixed value such as the mean corresponds to **an entirely different estimand** [@Hanmer2012].\nThe AME is a population-averaged quantity, corresponding to the population average treatment effect and these\ntwo quantities can produce very different results under fairly general conditions because **they do not answer \nthe same question**.\n\n::: a\nFrom here it is straightforward to obtain a model averaged marginal effect estimate for a discrete outcome model\nsuch as logit. Given marginal predictions for each model $k$ in the set $m$, we can apply equation @eq-logit-binary-bmame \nto obtain the posterior distribution of average marginal effects for each model.\n:::\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n&= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\n\\end{align}\n$$ {#eq-logit-binary-bmame} \n\nThis yields a model-wise mixture distribution representing the weighted average of the marginal effects\nestimates and has broad applicability to scenarios common in the social sciences, industry, and any \nother setting where a need arises to account for uncertainty in model specification.\n\n## Limitations and Caveats\n\nWhile traditional marginal likelihood-based Bayesian modeling is a powerful and principled \ntechnique for dealing with uncertainty in the process of model specification and selection, \nit is not without limitations and caveats of its own, some of which bear emphasizing before \nproceeding further.\n\n### Marginal Likelihood and Computational Uncertainty\n\nFirst, for all but the simplest of models attempting to derive the integral in equation \n@eq-marginal-likelihood analytically proves to be computationally intractable and we must \ninstead rely on algorithmic approximations such as bridge sampling [@Gronau2017; @Gelman1998; @Wang2020].\nWhile the bridge sampling approximation tends to perform well compared to common alternatives,\nestimates of the marginal likelihood $p(y \\,|\\, \\mathcal{M})$ obtained via bridge sampling may be \nhighly variable upon repeated runs of the algorithm [@Schad2022]. This problem tends to arise in more\ncomplex hierarchical models with multiple varying effects and, perhaps unsurprisingly, suggests\nthat by relying on approximations we introduce an additional source of computational uncertainty \nthat may need to be accounted for.\n\n::: a\nFor the purposes of BMAMEs, I have elsewhere proposed addressing this additional source of uncertainty \nand incorporating it into the estimates by executing the bridge sampling algorithm $S$ times for each \nmodel to obtain a distribution of possible values for the approximate log marginal likelihood. Given a \nvector of estimates for $p(y ~|~ \\mathcal{M})$ of length $s$ for each model $k \\in \\{1,2,\\dots, m\\}$, \nwe can apply equation @eq-posterior-probability to obtain an $s \\times m$ matrix of posterior model \nprobabilities. It is then straightforward to extend equation @eq-logit-binary-bmame average over the \ndistribution of posterior probability weights rather than relying on single estimate or point estimate \nas shown in equation @eq-logit-bmame-reps below.\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &=\\frac{1}{S}\\sum_{s=1}^{S} \\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z}\\right] \\\\\n&= \\frac{1}{S}\\sum_{s=1}^{S}\\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\\right]\n\\end{align}\n$$ {#eq-logit-bmame-reps} \n\nWhether this approach is necessary or not depends largely on how variable the algorithm is on repeated runs\nand it may simply be adequate to instead take the median or mean of the marginal likelihood estimates for\neach model. Although it is beyond the scope of this post, one could also use exhaustive leave-future-out \ncross-validation which is equivalent to the marginal likelihood given a logarithmic scoring rule \n[@Fong2020; @Buerkner2020], though this approach tends to be extremely expensive in terms of computation.\n:::\n\n### Bayes Factors and Prior Sensitivity\n\nThe second caveat with respect to obtaining BMAMEs stems from a common criticism of Bayes Factors. Although\nas the amount of available data $n \\longrightarrow \\infty$, the likelihood will tend to dominate the prior \nin Bayesian estimation, this is not necessarily the case in terms of inference. Indeed, from equations \n@eq-marginal-likelihood and @eq-posterior-probability one can quite clearly see the the posterior probability \n$\\Pr(\\mathcal{M} ~| ~y)$ depends on the prior probability of the parameters, $\\pi(\\theta \\,|\\, \\mathcal{M})$ \nin equation @eq-marginal-likelihood, and the respective prior probability $\\pi(\\mathcal{M})$ of each model \nin the set under consideration. Since our substantive conclusions may be sensitive to either of these prior \nspecifications, these assumptions should be checked to verify the results do not vary wildly under reasonable \nalternatives. \n\n::: a\nPerhaps more importantly, due to their dependence on Bayes Factors, posterior probability weights require at\nleast moderatlely informative priors on each and every parameter in a model and it is by this point common \nknowledge that specifying a flat or \"uninformative\" prior such as $\\beta \\sim \\mathrm{Uniform(-\\infty, \\infty)}$ \nor even an excessively vague one tends to bias Bayes Factors--and by extension values that depend on \nthem--violently in favor of the null model.[^3] This is a feature of applied Bayesian inference rather than a\nbug and **if you make stupid assumptions, you will end up with stupid results**. The solution to this problem \nis to think carefully about the universe of possible effect sizes you might observe for a given parameter, \nassign reasonable priors that constrain the parameter space, and verify that the results are robust to \nalternative distributional assumptions.\n\n[^3]: There is no such thing as an \"uninformative prior\"; the concept itself is not mathematically defined.\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Likeihoods and Priors\n\nThe degree to which a prior can be considered informative is heavily dependent on the likelihood and can often \nonly be understood in that context [@Gelman2017].\n:::\n\n### Stacking, and the Open-$\\mathcal{M}$ Problem\n\nFinally, traditional BMA is flawed in the sense that its validity rests upon the closed $\\mathcal{M}$ \nassumption implicit in equation @eq-posterior-probability. In the open $\\mathcal{M}$ setting where the \ntrue model is not among those in the set of candidate models under consideration, as will generally \nbe the case in any real world application, estimated effects based on posterior probability \nweights are likely to be biased [@Hollenbach2020; @Yao2018]. This problem arises from the fact that \nmarginal-likelihood based posterior probability weights for each model $m \\in \\mathcal{M}$ correspond to \nthe probability a given model captures the true data generation process and in most settings this is unlikely \nto ever be the case since, by definition, *all models are wrong*. \n\n::: a\nIn the open $\\mathcal{M}$ setting, traditional BMA will still assign each model some probability but since by \ndefinition $\\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} ~|~ y) = 1$, these weights no longer have a valid \ninterpretation in terms of the posterior probability a given model is true. Although this does not render marginal \nlikelihood based model averaging useless and it may often make more sense to think about model specification and \nselection as a probabilistic process aimed at identifying the most likely model conditional on the set of models \nunder consideration [@Hinne2020], it is advisable to assess whether our substantive conclusions are sensitive to \npotential violations of the closed $\\mathcal{M}$ assumption. This once again underscores the fact that statistics is \nnot an mechanistic truth generating process and *there is no magic*. It also underscores the need to think in more \nlocal terms--rather than asking *is this model true* we generally want to know *the probability $\\mathrm{X}$ \nbelongs to the true data generation process*.[^4]\n\n[^4]: As @Pluemper2018 demonstrate, BMA performs quite well in terms of this latter objective.\n\nAn alternative but still properly Bayesian approach aimed at addressing these potential flaws in traditional BMA \nis cross-validation based stacking [@Yao2018]. In contrast to traditional BMA, stacking \nand pseudo-BMA weights are constructed based on the relative expectation of the posterior predictive \ndensity [@Hollenbach2020; @Yao2018]. These approaches to estimating model weights do not rely upon the closed \n$\\mathcal{M}$ assumption, and thus provide a way of either avoiding it altogether or relaxing it as a robustness \ncheck on the posterior probability weights, albeit while answering a fundamentally different question more appropriate \nfor an open $\\mathcal{M}$ world. Since the overall approachg to calculating model averaged AMEs does not change\nand we are merely substituting the posterior probability weights in equation @eq-logit-binary-bmame with\nstacking or pseudo-BMA weights, I direct readers to @Yao2018 for a formal discussion of these approaches.\n::: \n\n# Implementation and Illustration\n\nOnce we have decided how to obtain the model weights--whether using marginal likelihood or cross-validation based \nstacking--the `{marginaleffects}` package provides the necessary functionality to handle everything else for us \nthanks to the feature-rich support for various approaches to averaging across posterior distributions provided by \n`{brms}`. To obtain the BMAME for a given parameter while accounting for the uncertainty in the model specifications, \nversion 0.5.0 and higher of `{marginaleffects}` allows users to specify the argument `type = \"average\"` in their\ncall to either `marginaleffects::marginaleffects` or `marginaleffects::comparisons` for objects of class `brmsfit`\nalong with any additional arguments to be passed down to `pp_average` such as the type of weights to estimate, or \nalternatively a numeric vector of pre-estimated weights which is usually the more computationally efficient option \nand the approach I take in the applied example below. For those cases not covered by the marginal effects \nimplementation, I also provide an illustration via R and Stan that should be fairly straightforward to convert to \nother languages as necessary.\n\n::: a\nTo demonstrate this functionality, I draw on an example loosely adapted from part of my dissertation research in \nwhich I examine how the active participation of women in rebel groups during wartime shapes the political calculus \nof former combatant groups at war's end--in short, I expect rebel groups where women participated in combat roles \nduring wartime are *more likely* to form political parties and participate in post-conflict elections. Note this \ndemonstration is not comprehensive and only illustrates one aspect of how BMAMEs might be applied in practice. \nHowever, as far as `{marginaleffects}` functionality is concerned, the general workflow is identical regardless.\n:::\n\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}