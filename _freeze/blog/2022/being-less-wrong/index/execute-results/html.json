{
  "hash": "6f8e653b48189bced72711b058437f9d",
  "result": {
    "markdown": "---\ntitle: The Art of Being Less Wrong\nsubtitle: An Introduction to Bayesian Model Averaged Marginal Effects\npagetitle: The Art of Being Less Wrong\ndate: 2022-05-24\ndescription: \"This post provides an introduction to Bayesian Model Averaging and Model Averaged Marginal Effects with Stan, `{brms}`, and `{marginaleffects}`\"\ncategories: [R, Bayesian Statistics, Causal Inference, Marginal Effects, Stan, Logit, Model Averaging]\nlicense: All content in this post is made available for public use under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License.\nimage: figs/preview-fig.png\ntoc-location: left\npage-layout: full\ncode-fold: true\n---\n\n\n\n\n::: callout-important\n### Work in Progress\n\nThis is a draft post and under is currently under development. I have not proofread any of the contents herein\n\n:::\n\n# Introduction\n\nGeorge E. P. Box famously described the problem of model development in statistics and related fields by noting, \n*all models are wrong, but some models are useful* [-@Box1976]. That is, all models are necessarily simplifications \nofa much more complex reality and statistics is by no means an algorithmic truth generating process because no \nmodel can ever be true in the pure sense of the word. Yet, this at times underappreciated reality does not render \nall models useless but rather implies the goal of those who practice statistics in its various forms is to \ndevelop and identify models that are useful in a never ending quest to be less wrong. In this post I introduce an \napproach to this task that has been the subject of my own dissertation work on the use of Bayesian Model Averaging \n(BMA) to account for uncertainty in model specification when estimating average marginal effects in non-linear \nregression models. \n\n::: a \nI introduce what I call a Bayesian Model Averaged Marginal Effect (BMAME) in the context of logistic regression \nmodels in political science and illustrate how to obtain BMAMEs in a straightforward manner using the \n`{marginaleffects}` R package [@ArelBundock2022a], which thanks to Vincent Arel-Bundock's assistance supports model averaged or\nstacked average marginal effects for any model fit with the Stan interface `{brms}` \n[@Burkner2017; @Burkner2018]. I also provide a more general demonstration in Stan that makes\nthe method easily applicable in Python and high dimensional settings. Although I focus here on models that include \nonly population-level effects, recent feature additions to `{marginaleffects}` now make it possible to obtain\nBMAMEs from more complex hierarchical models and I will cover such applications in a subsequent post.\n::: \n\n# Bayesian Model Averaging in Political Science\n\nAlthough Bayesian Model Averaging was introduced to political science more than two decades ago \nby @Bartels1997, cases of its use in political science remain rare and are largely confined to the topic \nof political methodology [@Juhl2019; @Cranmer2015; @Montgomery2010].[^1] This presents something of a \nproblem given the numerous studies demonstrating the dangers of tossing a large number of likely correlated \npredictors into a regression model [@Achen2005; @Clarke2005; @Clarke2009; @Montgomery2018]; the reality that \nif we wish to adjudicate between two or more competing theories, comparing coefficients in a single model is \ngenerally insufficient to accomplish such a task [@Imai2011; @Clarke2007; @Hollenbach2020]; and the difficulty \nof assessing what we truly known about political phenomenon that results from an obsession with \"statistical \nsignificance\" and precise answers to poorly defined questions [@Ward2010; @Schrodt2014; @Lundberg2021]. \n\n[^1]: For a recent application of BMA in the context of instrument selection see @Rozenas2019.\n\n::: a \nIn contrast to the advice of @Montgomery2010 who suggest \"BMA is best used as a subsequent robustness \ncheck to show that our inferences are not overly sensitive to plausible variations in model specification\" \n(266), I argue here that model averaging can and should be used as far more than a robustness check if for\nno other reason than because a model-wise mixture distribution for our paramter of intertest is almost \ncertainly more informative than a point estimate from a single model specification. Indeed, BMA provides\none of the few natural and intuitive ways of resolving some of the issues outlined in the preceding \nparagraph, particularly as it pertains to the evaluation of non-nested theoretical models, while also\nexhibiting a lower false positive rate than alternative approaches [@Pluemper2018]. This section provides \na brief explanation of BMA and its relation to what I term model averaged marginal effects before \nproceeding to a discussion of how the procedure is implemented.\n::: \n\n## Bayesian Model Comparison and Model Averaging\n\nConsider a simple case in which we have a set of $\\mathcal{M}$ models, each of which characterizes a \npossible representation of some unknown data generation process. Although it remains common practice \nin applied political science to select a single model based on a largely arbitrary fit statistic \n(i.e., AIC, $\\chi^{2}$, or whatever people are getting a p-value from this week), such an approach is \nproblematic since it ignores the uncertainty inherent in the process of model specification. Rather \nthan selecting a single model and consequently placing an implicit zero prior on \n**every other possible model specification you could have considered but did not**, Bayesian Model \nAveraging and its analogues provide a means of averaging across several possible, potentially \nnon-nested, model specifications and accounting for the uncertainty associated with doing so.\n\n::: a \nImagine we wish to adjudicate between two competing theoretical models in the set of candidate models \n$\\mathcal{M}$. The posterior odds of model $i$ relative to an alternative $k$ can be expressed as\n:::\n\n$$\n\\underbrace{\\frac{\\Pr(\\mathcal{M}_{i} \\, | \\, y)}{\\Pr(\\mathcal{M}_{k} \\, | \\, y)}}_{\\mathrm{Posterior~Odds}} = \\underbrace{\\frac{p(y \\, | \\, \\mathcal{M}_{i})}{p(y \\, | \\, \\mathcal{M}_{k})}}_{\\mathrm{Bayes~Factor}} \\times \\underbrace{\\frac{\\pi(\\mathcal{M}_{i})}{\\pi(\\mathcal{M}_{k})}}_{\\mathrm{Model~Prior}}\n$$ {#eq-posterior-odds} \n\nwhere $\\pi(\\mathcal{M})$ is the prior probability of model $i$ over model $k$ and $p(y \\,|\\, \\mathcal{M})$ \nis the marginal likelihood of the observed data under each of the candidate models such that\n\n$$\n\\underbrace{p(y \\,|\\, \\mathcal{M})}_{\\mathrm{Marginal~Likelihood}} = \\int\\underbrace{p(y \\,|\\, \\theta,\\, \\mathcal{M})}_{\\mathrm{Likelihood}} \\, \\underbrace{\\pi(\\theta \\,|\\, \\mathcal{M})}_{\\mathrm{Prior}}d\\theta\n$$ {#eq-marginal-likelihood}\n\n::: a \nGiven our prior assumptions about how likely the observed data are to have arisen from each model in the set \n$\\mathcal{M}$, the result of equation @eq-posterior-odds is the posterior odds of model $i$ compared to model \n$k$ and thus captures the relative probability that model $\\mathcal{M_{i}}$ represents the true data generation \nprocess compared to the plausible alternative $\\mathcal{M_{k}}$. We can easily extend this example to the setting \nin which $\\mathcal{M_{k}}$ is a set of plausible competing models of size $k > 2$, in which case the posterior \nmodel probability of the $i^{th}$ model relative to each of the alternatives $k$ is\n:::\n\n$$\n\\Pr(\\mathcal{M}_{i} \\,|\\, y) = \\frac{p(y \\, | \\, \\mathcal{M}_{i}) \\, \\cdot \\, \\pi(\\mathcal{M}_{i})}{\\displaystyle\\sum^{\\mathcal{m}}_{k=1} p(y \\, | \\, \\mathcal{M}_{k}) \\, \\cdot \\, \\pi(\\mathcal{M}_{k})}\n$$ {#eq-posterior-probability} \n\nwhich provides the posterior model probability of model $i$ relative to each of the alternative candidate \nspecifications $k$. Applying equation @eq-posterior-probability to each model in the set under consideration \nyields a vector of relative posterior model probabilities of length $m$ which may be used as either posterior \nprobability weights as in the case of Bayesian Model Averaging or for the estimation of posterior inclusion \nprobabilities.\n\n::: a \nIn the context of model averaging, we can take draws from the posterior predictive distribution of each model \ncontaining a given parameter of interest proportional to its posterior probability weight obtained from equation \n@eq-posterior-probability. This yields a model-wise mixture representing a weighted average of the posterior \npredictive distribution for some parameter such as a regression coefficient for a linear model or as I outline \nbelow, an average marginal effect or probability contrast for models that employ a non-linear link function. \n::: \n\n## Bayesian Model Averaged Marginal Effects\n\nAverage marginal effects are ubquitous in the social sciences though, as \n[Andrew Heiss nicely illustrates](https://www.andrewheiss.com/blog/2022/05/20/marginalia/), the term average \nmarginal effect is often used ambiguously and in the context of more complex models, the terms marginal and\nconditional [tend to be a source of additional confusion](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/). \nIn the case of a simple population-level linear model with a Gaussian likelihood and identity link function, \nthe model averaged marginal effect is equivalent to the posterior distribution of the model averaged \ncoefficient $\\beta$ which can be expressed as\n\n$$\n\\mathbb{E}(\\beta \\, | \\, y) = \\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} \\, | \\, y) \\cdot \\mathbb{E}(\\beta\\, |\\, \\mathcal{M}_{i}, \\, y) \\quad \\forall \\quad i \\in \\{1, 2,\\dots, m\\}\n$$ {#eq-bma-population} \n\nwhere $\\Pr(\\mathcal{M}_{i} ~ | ~ y)$ represent the posterior model probability and \n$\\mathbb{E}(\\beta ~|~ \\mathcal{M}_{i}, ~ y)$ is the expected value of some parameter $\\beta$ \nconditional on the observed data and some model specification for each model $i$ in the set of \nmodels under consideration $m$ [@Montgomery2010].\n\n::: a\nFor probability models such as logit, and other commonly used formulations for modeling\ndiscrete responses, this simple closed form solution does not exist nor is there any meaningful\nway to interpret the coefficients of these models as efffect size estimates--a reality that unfortunately \nremains quite widely misunderstood [@Mood2009; @Daniel2020]. As a general principle, any model more\ncomplex than a simple linear regression can only be meaningfully interpreted in terms of the predictions\nit generates [@Long2018], which in the case of logistic regression simply means applying the inverse \nlogistic function to obtain predicted probabilities which we can then use to estimate contrasts or average \nmarginal effects [@Norton2019]. Letting $\\mathrm{Y}$ represent a binary outcome, $\\mathrm{X}$ a continuous \nexposure or treatment of interest, and $\\mathrm{Z}$ a matrix of measured confounders, we can express the \naverage marginal effect as\n:::\n\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\frac{\\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij} + h, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = \\mathrm{x}_{ij}, \\mathrm{Z}_{ij}]d\\mathrm{Z}}{h} \\\\\n$$ {#eq-logit-continuous-ame} \n\nwhich provided a sufficiently small value of $h$, yields a reasonable approximation of the partial derivative \nand provides a posterior distribution for the average effect of an instantaneous change in $\\mathrm{X}$ on the\nprobability scale similar to what one obtains in linear regression.[^2]\n\n[^2]: In the case of an interval exposure, it may make more sense to estimate a marginal effect at each unique or representative value of $\\mathrm{X}$ to better capture non-linearities.\n\n::: a\nSimilarly, for a binary treatment of interest we apply the Bayesian g-formula to obtain the average marginal \neffect of $\\mathrm{X}$ on the probability of $\\mathrm{Y}$ as shown in equation @eq-logit-binary-ame.\n:::\n$$\n\\mathrm{AME}\\Delta_{j} = \\int \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij} ~ | ~ \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n$$ {#eq-logit-binary-ame}\n\nEquations @eq-logit-continuous-ame and @eq-logit-binary-ame make clear that we are averaging over the confounder \ndistribution $\\mathrm{Z}$ rather than holding it constant at some fixed value [@Oganisian2020; @Keil2017]. \nThis point is an important one because in binary outcome models such as logit and probit, holding $\\mathrm{Z}$ \nconstant at some fixed value such as the mean corresponds to **an entirely different estimand** [@Hanmer2012].\nThe AME is a population-averaged quantity, corresponding to the population average treatment effect and these\ntwo quantities can produce very different results under fairly general conditions because **they do not answer \nthe same question**.\n\n::: a\nFrom here it is straightforward to obtain a model averaged marginal effect estimate for a discrete outcome model\nsuch as logit. Given marginal predictions for each model $k$ in the set $m$, we can apply equation @eq-logit-binary-bmame \nto obtain the posterior distribution of average marginal effects for each model.\n:::\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z} \\\\\n&= \\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\n\\end{align}\n$$ {#eq-logit-binary-bmame} \n\nThis yields a model-wise mixture distribution representing the weighted average of the marginal effects\nestimates and has broad applicability to scenarios common in the social sciences, industry, and any \nother setting where a need arises to account for uncertainty in model specification.\n\n## Limitations and Caveats\n\nWhile traditional marginal likelihood-based Bayesian modeling is a powerful and principled \ntechnique for dealing with uncertainty in the process of model specification and selection, \nit is not without limitations and caveats of its own, some of which bear emphasizing before \nproceeding further.\n\n### Marginal Likelihood and Computational Uncertainty\n\nFirst, for all but the simplest of models attempting to derive the integral in equation \n@eq-marginal-likelihood analytically proves to be computationally intractable and we must \ninstead rely on algorithmic approximations such as bridge sampling [@Gronau2017; @Gelman1998; @Wang2020].\nWhile the bridge sampling approximation tends to perform well compared to common alternatives,\nestimates of the marginal likelihood $p(y \\,|\\, \\mathcal{M})$ obtained via bridge sampling may be \nhighly variable upon repeated runs of the algorithm [@Schad2022]. This problem tends to arise in more\ncomplex hierarchical models with multiple varying effects and, perhaps unsurprisingly, suggests\nthat by relying on approximations we introduce an additional source of computational uncertainty \nthat may need to be accounted for.\n\n::: a\nFor the purposes of BMAMEs, I have elsewhere proposed addressing this additional source of uncertainty \nand incorporating it into the estimates by executing the bridge sampling algorithm $S$ times for each \nmodel to obtain a distribution of possible values for the approximate log marginal likelihood. Given a \nvector of estimates for $p(y ~|~ \\mathcal{M})$ of length $s$ for each model $k \\in \\{1,2,\\dots, m\\}$, \nwe can apply equation @eq-posterior-probability to obtain an $s \\times m$ matrix of posterior model \nprobabilities. It is then straightforward to extend equation @eq-logit-binary-bmame average over the \ndistribution of posterior probability weights rather than relying on single estimate or point estimate \nas shown in equation @eq-logit-bmame-reps below.\n\n$$\n\\begin{align}\n\\mathrm{BMAME}\\Delta_{j} &=\\frac{1}{S}\\sum_{s=1}^{S} \\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\int \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 1, \\mathrm{Z}_{ij}] ~-~ \\mathbb{E}[\\mathrm{Y}_{ij}, \\mathcal{M}_{k} | \\mathrm{X}_{ij} = 0, \\mathrm{Z}_{ij}]d\\mathrm{Z}\\right] \\\\\n&= \\frac{1}{S}\\sum_{s=1}^{S}\\left[\\sum_{k=1}^{m} \\Pr(\\mathcal{M}_{k, s} | y)\\cdot\\mathrm{AME}\\Delta_{k,j}\\right]\n\\end{align}\n$$ {#eq-logit-bmame-reps} \n\nWhether this approach is necessary or not depends largely on how variable the algorithm is on repeated runs\nand it may simply be adequate to instead take the median or mean of the marginal likelihood estimates for\neach model. Although it is beyond the scope of this post, one could also use exhaustive leave-future-out \ncross-validation which is equivalent to the marginal likelihood given a logarithmic scoring rule \n[@Fong2020; @Buerkner2020], though this approach tends to be extremely expensive in terms of computation.\n:::\n\n### Bayes Factors and Prior Sensitivity\n\nThe second caveat with respect to obtaining BMAMEs stems from a common criticism of Bayes Factors. Although\nas the amount of available data $n \\longrightarrow \\infty$, the likelihood will tend to dominate the prior \nin Bayesian estimation, this is not necessarily the case in terms of inference. Indeed, from equations \n@eq-marginal-likelihood and @eq-posterior-probability one can quite clearly see the the posterior probability \n$\\Pr(\\mathcal{M} ~| ~y)$ depends on the prior probability of the parameters, $\\pi(\\theta \\,|\\, \\mathcal{M})$ \nin equation @eq-marginal-likelihood, and the respective prior probability $\\pi(\\mathcal{M})$ of each model \nin the set under consideration. Since our substantive conclusions may be sensitive to either of these prior \nspecifications, these assumptions should be checked to verify the results do not vary wildly under reasonable \nalternatives. \n\n::: a\nPerhaps more importantly, due to their dependence on Bayes Factors, posterior probability weights require at\nleast moderatlely informative priors on each and every parameter in a model and it is by this point common \nknowledge that specifying a flat or \"uninformative\" prior such as $\\beta \\sim \\mathrm{Uniform(-\\infty, \\infty)}$ \nor even an excessively vague one tends to bias Bayes Factors--and by extension values that depend on \nthem--violently in favor of the null model.[^3] This is a feature of applied Bayesian inference rather than a\nbug and **if you make stupid assumptions, you will end up with stupid results**. The solution to this problem \nis to think carefully about the universe of possible effect sizes you might observe for a given parameter, \nassign reasonable priors that constrain the parameter space, and verify that the results are robust to \nalternative distributional assumptions.\n\n[^3]: There is no such thing as an \"uninformative prior\"; the concept itself is not mathematically defined.\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Likeihoods and Priors\n\nThe degree to which a prior can be considered informative is heavily dependent on the likelihood and can often \nonly be understood in that context [@Gelman2017].\n:::\n\n### Stacking, and the Open-$\\mathcal{M}$ Problem\n\nFinally, traditional BMA is flawed in the sense that its validity rests upon the closed $\\mathcal{M}$ \nassumption implicit in equation @eq-posterior-probability. In the open $\\mathcal{M}$ setting where the \ntrue model is not among those in the set of candidate models under consideration, as will generally \nbe the case in any real world application, estimated effects based on posterior probability \nweights are likely to be biased [@Hollenbach2020; @Yao2018]. This problem arises from the fact that \nmarginal-likelihood based posterior probability weights for each model $m \\in \\mathcal{M}$ correspond to \nthe probability a given model captures the true data generation process and in most settings this is unlikely \nto ever be the case since, by definition, *all models are wrong*. \n\n::: a\nIn the open $\\mathcal{M}$ setting, traditional BMA will still assign each model some probability but since by \ndefinition $\\sum_{i=1}^{m}\\Pr(\\mathcal{M}_{i} ~|~ y) = 1$, these weights no longer have a valid \ninterpretation in terms of the posterior probability a given model is true. Although this does not render marginal \nlikelihood based model averaging useless and it may often make more sense to think about model specification and \nselection as a probabilistic process aimed at identifying the most likely model conditional on the set of models \nunder consideration [@Hinne2020], it is advisable to assess whether our substantive conclusions are sensitive to \npotential violations of the closed $\\mathcal{M}$ assumption. This once again underscores the fact that statistics is \nnot an mechanistic truth generating process and *there is no magic*. It also underscores the need to think in more \nlocal terms--rather than asking *is this model true* we generally want to know *the probability $\\mathrm{X}$ \nbelongs to the true data generation process*.[^4]\n\n[^4]: As @Pluemper2018 demonstrate, BMA performs quite well in terms of this latter objective.\n\nAn alternative but still properly Bayesian approach aimed at addressing these potential flaws in traditional BMA \nis cross-validation based stacking [@Yao2018]. In contrast to traditional BMA, stacking \nand pseudo-BMA weights are constructed based on the relative expectation of the posterior predictive \ndensity [@Hollenbach2020; @Yao2018]. These approaches to estimating model weights do not rely upon the closed \n$\\mathcal{M}$ assumption, and thus provide a way of either avoiding it altogether or relaxing it as a robustness \ncheck on the posterior probability weights, albeit while answering a fundamentally different question more appropriate \nfor an open $\\mathcal{M}$ world. Since the overall approachg to calculating model averaged AMEs does not change\nand we are merely substituting the posterior probability weights in equation @eq-logit-binary-bmame with\nstacking or pseudo-BMA weights, I direct readers to @Yao2018 for a formal discussion of these approaches.\n::: \n\n# Female Combatants and Post-Conflict Rebel Parties\n\nTo illustrate the utility of BMAMEs in political science, I draw on an example adapted from the second chapter of\nmy dissertation in which I examine how the active participation of women in rebel groups during wartime shapes \nthe political calculus of former combatant groups at war's end. In short, I expect rebel groups where women \nparticipated in combat roles during wartime are *more likely* to form political parties and participate in \npost-conflict elections. \n\n## Data\n\nTo identify the universe of post-conflict successor parties to former combatant groups I rely on the Civil \nWar Successor Party (CWSP) dataset which includes all civil wars in the UCDP Armed Conflicts Dataset (UCDP) \nthat terminated between 1970 and 2015 [@Daly2020]. Since the primary distinction between political parties \nand other civil society organizations such as interest groups lies in the latter's participation in elections \nand fielding candidates for political office and a party that does not compete in elections is no party at all, \nI operationalize our dependent variable, *Rebel to Party Transition*, as a binary event which takes a value of \n1 if a rebel group both forms a political party and directly participates in a country's first post-conflict \nelections and 0 otherwise. The resulting sample consists of 112 observations for 108 unique rebel groups \nacross 56 countries, approximately 66% of which formed parties and participated in their country's first \npost-conflict election.\n\n::: a\nFollowing @Wood2017, I define female combatants as members of a rebel group who are \"armed and participate in \norganized combat activities on behalf of a rebel organization\" (38). I code instances of female participation \nin rebel groups by mapping version 1.4 of the Women in Armed Rebellion Data (WARD) to the CWSP and construct \ntwo versions of the main predictor of interest based on the best estimate for the presence of female combatants \nin a given rebel group, one of which includes cases in which women served only as suicide bombers while \nthe other codes these groups as not having any women in combat positions. Each of these variables takes a value \nof 1 if women participated in combat in a given rebel group and 0 otherwise.[^5] Among the rebel groups in our \nanalysis, female combatants have been active in some capacity in 52.67% of those who were involved in civil wars \nthat have terminated since 1970 and 50.89% featured women in non-suicide bombing roles.\n\nI also adjust for several additional factors that may otherwise confound the estimate of the effect of \nfemale participation in combat on rebel successor party transitions. First, insurgent groups that attempt \nto perform functions typically characteristic of the state during wartime may be more likely to transition \nto political parties following the end of a conflict since, in doing so, they both develop organizational \ncapacity and if successful, demonstrate that they are capable of governing to their future base of support. \nMoreover, as rebel groups build organizational capacity and accumulate institutional knowledge, their ability \nto sustain a successful insurgency and recruit combatants grows as well. To adjust for rebel organizational \ncapacity I specify a Bayesian multidimensional item response model to construct a latent scale capturing the \ndegree to which rebel groups engaged in functions of the state during wartime based on twenty-six items from \nQuasi-State Institutions dataset [@Albert2022].\n\nSecond, I adjust for baseline characteristics of each rebel group that previous research suggests influences \nboth where women rebel and post-conflict party transitions. Drawing on data from @Braithwaite2019, I construct \na series of dichotomous indicator variables for whether a rebel group is affiliated with an existing political \nparty [@Manning2016], nationalist and left-wing or communist ideological origins [@Wood2017], and whether a \ngroup has ties to a specific ethnic community since this may provide advantages in terms of mobilizing \ncombatants and voters along specific cleavages. I also construct two additional indictators to capture \nwhether the United Nations intervened in the conflict and whether a given rebel group was located in Africa.\n\nWhile it is, of course, impossible in the context of an observational setting such as this one to isolate\nevery possible source of confounding, nor is it generally advisable to attempt to do so, I focus on \nthese baseline covariates since existing research provides a strong theoretical basis to suspect they \nimpact the relationship of interest.[^5] For the purposes of this illustration, I assume this set of covariates\nis sufficient to satisfy strong ignorability but such an assumption is likely unreasonable and \nthe particular analysis here is ultimately best taken as descriptive in nature.\n:::\n\n[^5]: Those familiar with the post-conflict rebel parties literature in political science are likely rolling their eyes at this sentence because in its present state it is not clear we know much of anything about causal relationships.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Read in the pre-processed data\nmodel_df <- read_parquet(\"data/model_data.gz.parquet\")\n```\n:::\n\n\n\n## Model and Priors\n\nTo estimate the effect of women's participation in combat on post-conflict rebel party transitions for \nthis example, I specify a simple logit model as shown in equation @eq-logit-model. The bernoulli \nprobability $\\theta$ of a rebel group transitioning to a political party at war's end is modeled as a \nfunction of the population-level intercept $\\alpha$ and a vector of coefficients $\\beta_{k}$ corresponding \nto the matrix of inputs $X_{n}$.\n\n$$\n\\begin{align}\ny_{i} &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n\\mathrm{logit}(\\theta) &= \\alpha + X_{n}\\beta_{k}\\\\\n\\text{with priors}\\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 1)\\\\\n\\beta_{k} &\\sim \\mathrm{Normal}(0, 0.75)\n\\end{align}\n$$ {#eq-logit-model}\n\n::: a\nSince we model $\\theta$ via a logit link function, our priors on the intercept $\\alpha$ and \ncoefficients $\\beta_{k}$ should correspond to the expected change in the log odds of the response \nfor each predictor. Some may find the process of prior elicitation in logistic regression models \ndifficult since reasoning on the logit scale may seem unintuitive. As figure @fig-logit-scale\nillustrates, the the log odds of the response for models with that employ a logit link function \nis effectively bounded between $\\{-6, 6\\}$ and values in excess of $|5|$ generally should not \noccur unless your data exhibits near complete or quasi-complete separation.\n:::\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n## Simulate logit probabilities\nlogit_scale <- tibble(\n  x = seq(-6.5, 6.5, 0.0001),\n  y = brms::inv_logit_scaled(x)\n)\n\n## Plot the logit scale\nggplot(logit_scale, aes(x = x, y = y)) +\n  ## Add a line geom\n  geom_line(size = 3, color = \"firebrick\") +\n  ## Add labels\n  labs(x = \"Log Odds\", y = \"Probability\") +\n  ## Adjust the x axis scale breaks\n  scale_x_continuous(breaks = seq(-6, 6, 1)) +\n  ## Adjust the y axis scale breaks\n  scale_y_continuous(breaks = seq(0, 1, 0.1))\n```\n\n::: {.cell-output-display}\n![Scale of the Logit Link](figs/fig-logit-scale.svg){#fig-logit-scale fig-align='center' width=1344}\n:::\n:::\n\n::: a\nFramed in terms of effect sizes, a change of $\\pm 3$ in the log odds of the response relative to the baseline spans half the logit scale in either direction. In the social sciences where data is largely random noise and effect sizes are typically small to moderate at best and while the road to hell is paved with *rules of thumb*, we can generally assume *a priori* that 1) any true effect of a given parameter $\\beta$ in the population is likely modest in magnitude and 2) values closer zero are more likely than those further away. From these relatively straightforward assumptions, we can arrive at a prior for the coefficients that follows a normal distribution with mean 0 and scale $0.75$ as shown in equation @eq-logit-model. In practical terms, this assumes the universe possible effect sizes for each predictor is normally distributed in the range of approximately $-2.5$ and $2.5$ on the logit scale. \n\nFor the population-level intercept $\\alpha$ I assign a slightly more diffuse normal prior mean 0 and scale 1 or $\\alpha \\sim \\text{Normal}(0,~1)$ which reflects a slightly greater degree of uncertainty about the baseline probability but is still moderately informative and constrains the bulk of the prior density in the range of realistic values. It is important to note here that as shown in Stan model below, we are specifying a prior on the intercept for the centered inputs rather than on the original scale. To more explicitly communicate our prior assumptions, we can use the tools for visualizing uncertainty provided in the `{ggdist}` package as shown in figure 2 below [@Kay2022].\n:::\n\n\n::: {.cell layout-align=\"center\" fig.dpi='300'}\n\n```{.r .cell-code}\n## Simulate the prior probabilities\nlogit_priors <- tibble(\n  alpha_prior = rnorm(10e5, 0, 1),\n  beta_prior = rnorm(10e5, 0, 0.75)\n) %>% \n  ## Pivot the simulated data to long form\n  pivot_longer(everything())\n\n### Set parameter names for the facets----\nlogit_prior_labels <- as_labeller(\n  x = c(\n    \"beta_prior\" = \"bolditalic(beta[k] * phantom(.) * phantom() %~% phantom() * phantom(.) * Normal(0*list(,) * phantom(.) * 0.75))\",\n    \"alpha_prior\" = \"bolditalic(alpha[k] * phantom(.) * phantom() %~% phantom() * phantom(.) * Normal(0*list(,) * phantom(.) * 1))\"\n  ), \n  default = label_parsed\n)\n\n## Plot the prior distributions\nlogit_prior_plots <- logit_priors %>% \n  # Initiate the ggplot object\n  ggplot(aes(y = value, fill = name, shape = name)) +\n  # Facet by the prior distributions\n  facet_wrap(~ name, scales = \"free\", labeller = logit_prior_labels) +\n  # Add a half eye geom for the prior distributions\n  stat_halfeye(\n    aes(slab_alpha = stat(pdf)),\n    .width = c(.5, .8, .95),\n    fill_type = \"segments\",\n    trim = FALSE,\n    show.legend = FALSE\n  ) +\n  # Manually set the fill colors\n  scale_fill_manual(values = c(\"#208820\", \"#0060B0\")) +\n  # Manually set the shapes\n  scale_shape_manual(values = c(22, 24)) +\n  # Adjust the breaks on the y axis\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +\n  # Flip the x and y axis\n  coord_flip() + \n  # Add labels to the plot\n  labs(\n    x = \"Prior Probability Density\",\n    y = latex2exp::TeX(r'(Log $\\Pr(\\theta_{prior})$)', bold = T)\n  )\n\n## Print the combined plot, recommended that you use ggsave\nlogit_prior_plots\n```\n\n::: {.cell-output-display}\n![Scale of the Logit Link](figs/fig-logit-priors.png){#fig-logit-priors fig-align='center' width=1920}\n:::\n:::\n\n\n::: a\nSince the model here is computationally quite efficient--on average, each model takes approximately 3 seconds to converge with 40,000 post-warmup draws--and in the interest of completeness, I search the full parameter space which consists of $k^{2}$ possible model specifications. Although it is possible to do this in this context, researchers will often instead need to place constrain the number of possible model configurations, generally by specifying an implicit prior model probability of 1 for certain parameters [@Montgomery2010; @Hinne2020]. This brings us to an unfortunate limitation of `{brms}` which requires recompiling the underlying Stan model for each fit, a process that ends up being extremely inefficient for a large number of model configurations since each model takes three times as long to compile as it does to fit. \n\nTo circumvent this issue, I implement the model directly in Stan as shown below which is designed to be directly compatible with brms and implments the bayesian g-formula for the average marginal effect of women's participation in combat on the probability of a rebel-to-party transition during the estimation process in the generated quanitites block. As I previously demonstrated in [this post](../statistical-adjustment-interpretation/index.qmd), it is then possible to compile the Stan model once and use the same program for each dataset using `{cmdstanr}` directly. Further computational gains can be obtained by using the `{furrr}` package to fit multiple models in parallel.\n:::\n\n::: panel-tabset\n\n### g-formula\n```stan\n// Bayesian Logit Model with Optional Bootstrapped AMEs via the Bayesian g-formula\ndata {\n  int<lower=1> N; // N Observations\n  array[N] int Y; // Response Variable\n  int<lower=1> K; // Number of Population-Level Effects\n  matrix[N, K] X; // Population-level Design Matrix\n  int prior_only; // Should the likelihood be ignored?\n  int treat_pos; // Data for the bootstrapped AMEs\n}\n\ntransformed data {\n\n  int Kc = K - 1;\n  matrix[N, Kc] Xc; // centered version of X without an intercept\n  vector[Kc] means_X; // column means of X before centering\n\n  for (i in 2 : K) {\n    means_X[i - 1] = mean(X[ : , i]);\n    Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n  }\n  \n  // Bootstrap Probabilities\n  vector[N] boot_probs = rep_vector(1.0/N, N);\n  \n  // Matrices for the bootstrapped predictions\n  matrix[N, Kc] Xa; \n  matrix[N, Kc] Xb;\n    \n  // Potential Outcome Y(X = 1, Z)\n  Xa = X[, 2:K];\n  Xa[, treat_pos - 1] = ones_vector(N);\n    \n  // Potential Outcome Y(X = 0, Z)\n  Xb = X[, 2:K];\n  Xb[, treat_pos - 1] = zeros_vector(N);\n}\n\nparameters {\n  vector[Kc] b; // Regression Coefficients\n  real Intercept; // Centered Intercept\n}\n\ntransformed parameters {\n  real lprior = 0; // Prior contributions to the log posterior\n  lprior += normal_lpdf(b | 0, 0.75);\n  lprior += normal_lpdf(Intercept | 0, 1);\n}\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | X, Intercept, b);\n  }\n\n  // priors including constants\n  target += lprior;\n}\n\ngenerated quantities {\n\n  // Actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n\n  // Additionally sample draws from priors\n  array[Kc] real prior_b = normal_rng(0, 0.75);\n  real prior_Intercept = normal_rng(0, 1);\n  \n  // Row index to be sampled for bootstrap\n  int row_i;\n    \n  // Calculate Average Marginal Effect in the Bootstrapped sample\n  real AME = 0;\n  array[N] real Y_X1; // Potential Outcome Y(X = 1, Z)\n  array[N] real Y_X0; // Potential Outcome Y(X = 0, Z)\n    \n  for (n in 1:N) {\n    // Sample the Baseline Covariates\n    row_i = categorical_rng(boot_probs);\n      \n    // Sample Y(x) where x = 1 and x = 0\n    Y_X1[n] = bernoulli_logit_rng(b_Intercept + Xa[row_i] * b);\n    Y_X0[n] = bernoulli_logit_rng(b_Intercept + Xb[row_i] * b);\n      \n    // Add Contribution of the ith Observation to the Bootstrapped AME\n    AME = AME + (Y_X1[n] - Y_X0[n])/N;\n  }\n    \n  // Take the mean of the posterior expectations\n  real EYX1 = mean(Y_X1); // E[Y | X = 1, Z]\n  real EYX0 = mean(Y_X0); // E[Y | X = 0, Z]\n}\n```\n\n### Reference\n\n```stan\n// Bayesian Logit Model for Models without the Treatment Variable\ndata {\n  int<lower=1> N; // total number of observations\n  array[N] int Y; // response variable\n  int<lower=1> K; // number of population-level effects\n  matrix[N, K] X; // population-level design matrix\n  int prior_only; // should the likelihood be ignored?\n}\n\ntransformed data {\n\n  int Kc = K - 1;\n  matrix[N, Kc] Xc; // centered version of X without an intercept\n  vector[Kc] means_X; // column means of X before centering\n\n  for (i in 2 : K) {\n    means_X[i - 1] = mean(X[ : , i]);\n    Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n  }\n}\n\nparameters {\n  vector[Kc] b; // Regression Coefficients\n  real Intercept; // Centered Intercept\n}\n\ntransformed parameters {\n  real lprior = 0; // Prior contributions to the log posterior\n  lprior += normal_lpdf(b | 0, 0.75);\n  lprior += normal_lpdf(Intercept | 0, 1);\n}\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | X, Intercept, b);\n  }\n\n  // priors including constants\n  target += lprior;\n}\n\ngenerated quantities {\n\n  // Actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n\n  // Additionally sample draws from priors\n  array[Kc] real prior_b = normal_rng(0, 0.75);\n  real prior_Intercept = normal_rng(0, 1);\n}\n```\n:::\n\n\n# Implementation and Illustration\n\nOnce we have decided how to obtain the model weights--whether using marginal likelihood or cross-validation based \nstacking--the `{marginaleffects}` package provides the necessary functionality to handle everything else for us \nthanks to the feature-rich support for various approaches to averaging across posterior distributions provided by \n`{brms}`. To obtain the BMAME for a given parameter while accounting for the uncertainty in the model specifications, \nversion 0.5.0 and higher of `{marginaleffects}` allows users to specify the argument `type = \"average\"` in their\ncall to either `marginaleffects::marginaleffects` or `marginaleffects::comparisons` for objects of class `brmsfit`\nalong with any additional arguments to be passed down to `pp_average` such as the type of weights to estimate, or \nalternatively a numeric vector of pre-estimated weights which is usually the more computationally efficient option \nand the approach I take in the applied example below. For those cases not covered by the marginal effects \nimplementation, I also provide an illustration via R and Stan that should be fairly straightforward to convert to \nother languages as necessary.\n\n::: a\n\n:::\n\n## Inclusion Bayes Factors\n\nIn a Bayesian framework, model comparison is formalized by means of posterior model probabilities as shown in equation @eq-posterior-probability. Given $m$ possible candidate models and letting $i$ denote those containing the parameter of interest and $k$ those that do not, we can obtain an inclusion Bayes Factor that represents the relative posterior odds of the parameter of interest. To briefly illustrate this, imagine we have a set of six models $\\mathcal{M}$ in which models $i \\in \\{1, 2, 3\\}$ contain the parameter of interest while models $k \\in \\{4, 5, 6\\}$ do not. The relative odds of a model containing a given parameter is simply the sum of the posterior model probabilities for each model $\\mathcal{M_{i}}$ over $\\mathcal{M_{k}}$ as shown in\n\n$$\n\\mathrm{BF_{Inclusion}}~=~\\frac{\\sum_{i=1}\\Pr(\\mathcal{M}_{i} \\, | \\, y)}{\\sum_{k=1}\\Pr(\\mathcal{M}_{k} \\, | \\, y)}\n$$ {#eq-inclusion-prob}\n\nAlthough my interest in this manuscript lies primarily in using the posterior model probabilities to obtain a model-wise mixture distribution of the substantive effect estimates as detailed in the following section and the inclusion Bayes Factors are simply presented for completeness, this briefly illustrates how one can formally test a hypothesis in a Bayesian framework and thus obtain a vague answer to a precise question as opposed to the common approach of providing precise answer to the wrong one [@Gross2014; @Kruschke2017].\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}